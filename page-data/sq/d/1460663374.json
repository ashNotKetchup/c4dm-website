{"data":{"about":{"html":"<p>The <em><strong>Centre for Digital Music</strong></em> is a world-leading, multidisciplinary research group in the field of music &#x26; audio technology.</p>\n<!-- <p style=\"color: red\">**THIS SEEMS TO BE THE ONLY SUPPORTED WAY OF IMPORTING TWITTER TWEETS IN GATSBY 5 AT THE MOMENT, AFAIK IT MEANS THAT EACH \nTWEET HAS TO BE MANUALLY IMPORTED INTO AN MD FILE...**</p>\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">This is what we do: <a href=\"https://t.co/pkyS6IcIUy\">https://t.co/pkyS6IcIUy</a> - an excellent video intro to the wonderful researchers of <a href=\"https://twitter.com/c4dm?ref_src=twsrc%5Etfw\">@c4dm</a>. <a href=\"https://twitter.com/QMEECS?ref_src=twsrc%5Etfw\">@QMEECS</a> <a href=\"https://twitter.com/QMUL?ref_src=twsrc%5Etfw\">@QMUL</a> <a href=\"https://twitter.com/hashtag/research?src=hash&amp;ref_src=twsrc%5Etfw\">#research</a></p>&mdash; C4DM at QMUL (@c4dm) <a href=\"https://twitter.com/c4dm/status/857989625922695173?ref_src=twsrc%5Etfw\">April 28, 2017</a></blockquote> -->","frontmatter":{"title":"About the centre","video":"https://www.youtube.com/embed/Rcbs4NvMFHM","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png","srcSet":"/static/c138fbce66e709a3f503405435de2f2c/0fc07/c4dm.png 100w,\n/static/c138fbce66e709a3f503405435de2f2c/bc685/c4dm.png 200w,\n/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png 400w,\n/static/c138fbce66e709a3f503405435de2f2c/7ef25/c4dm.png 800w","sizes":"(min-width: 400px) 400px, 100vw"},"sources":[{"srcSet":"/static/c138fbce66e709a3f503405435de2f2c/29e9d/c4dm.webp 100w,\n/static/c138fbce66e709a3f503405435de2f2c/2f9ed/c4dm.webp 200w,\n/static/c138fbce66e709a3f503405435de2f2c/ece87/c4dm.webp 400w,\n/static/c138fbce66e709a3f503405435de2f2c/4bc26/c4dm.webp 800w","type":"image/webp","sizes":"(min-width: 400px) 400px, 100vw"}]},"width":400,"height":60}}}}},"projects":{"nodes":[{"fields":{"slug":"/projects/cdtdata"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080838","images":{"fallback":{"src":"/static/bd2eecf83bf5392a4ac7061919a52914/cd63e/cdtdata.png","srcSet":"/static/bd2eecf83bf5392a4ac7061919a52914/08744/cdtdata.png 145w,\n/static/bd2eecf83bf5392a4ac7061919a52914/3c29b/cdtdata.png 290w,\n/static/bd2eecf83bf5392a4ac7061919a52914/cd63e/cdtdata.png 579w","sizes":"(min-width: 579px) 579px, 100vw"},"sources":[{"srcSet":"/static/bd2eecf83bf5392a4ac7061919a52914/72079/cdtdata.webp 145w,\n/static/bd2eecf83bf5392a4ac7061919a52914/a7f7a/cdtdata.webp 290w,\n/static/bd2eecf83bf5392a4ac7061919a52914/ba6fb/cdtdata.webp 579w","type":"image/webp","sizes":"(min-width: 579px) 579px, 100vw"}]},"width":579,"height":579}}},"title":"Centre for Doctoral Training (CDT) in Data Centric Engineering","author":"Prof Eram Rizvi (PI), Prof Mark Sandler (CI), Prof Nick Bryan-Kinns (CI)","date":null},"html":"","id":"e68ade9f-702f-5695-9a7d-e2ab94af8c72"},{"fields":{"slug":"/projects/Fazekas-Sony"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"New Methodologies for Efficient and Controllable Music Generation","author":"Dr George Fazekas (PI)","date":null},"html":"","id":"a3e9f3a1-9edd-50fe-b941-283fe6f7de85"},{"fields":{"slug":"/projects/Fazekas-UMG2"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Beyond Supervised Learning for Musical Audio","author":"Dr George Fazekas (PI)","date":null},"html":"","id":"433744e4-ecdf-538d-903d-85748cab2ea6"},{"fields":{"slug":"/projects/aimcdt"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/98704461ac8eeac7d8b8a64dc1520bf4/7a590/aimcdt.png","srcSet":"/static/98704461ac8eeac7d8b8a64dc1520bf4/4b686/aimcdt.png 154w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/d2213/aimcdt.png 308w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/7a590/aimcdt.png 615w","sizes":"(min-width: 615px) 615px, 100vw"},"sources":[{"srcSet":"/static/98704461ac8eeac7d8b8a64dc1520bf4/b2942/aimcdt.webp 154w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/46581/aimcdt.webp 308w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/fa942/aimcdt.webp 615w","type":"image/webp","sizes":"(min-width: 615px) 615px, 100vw"}]},"width":615,"height":615}}},"title":"UKRI Centre for Doctoral Training in Artificial Intelligence and Music (AIM)","author":"Prof Simon Dixon (PI), Dr Mathieu Barthet (CI), Dr Nick Bryan-Kinns (CI), Dr Gyorgy Fazekas (CI), Prof Mark Sandler (CI), Dr Andrew McPherson (CI), Dr Emmanouil Benetos (CI)","date":null},"html":"","id":"237af1c8-14dc-5167-a5e1-fa55e6860dea"},{"fields":{"slug":"/projects/Dixon-DAACI"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Performance Rendering for Music Generation Systems","author":"Prof Simon Dixon (PI)","date":null},"html":"","id":"780d9bfc-fda8-5320-aa0c-0a93788ae549"},{"fields":{"slug":"/projects/ZBenetos-Spotify"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Style classification of podcasts using audio","author":"Dr Emmanouil Benetos (PI)","date":null},"html":"","id":"19d1254c-f423-52ab-ae78-51e201f32ab6"}]},"news":{"nodes":[{"fields":{"slug":"/news/2024-05-29.C4DM-PhD_students_organise_DCASE_task"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#282828","images":{"fallback":{"src":"/static/1778669a3aa300652155162f4abc6354/0a9b5/DCASE.jpg","srcSet":"/static/1778669a3aa300652155162f4abc6354/384e6/DCASE.jpg 292w,\n/static/1778669a3aa300652155162f4abc6354/75c79/DCASE.jpg 584w,\n/static/1778669a3aa300652155162f4abc6354/0a9b5/DCASE.jpg 1167w","sizes":"(min-width: 1167px) 1167px, 100vw"},"sources":[{"srcSet":"/static/1778669a3aa300652155162f4abc6354/ad81e/DCASE.webp 292w,\n/static/1778669a3aa300652155162f4abc6354/4e6da/DCASE.webp 584w,\n/static/1778669a3aa300652155162f4abc6354/c3035/DCASE.webp 1167w","type":"image/webp","sizes":"(min-width: 1167px) 1167px, 100vw"}]},"width":1167,"height":1167}}},"title":"C4DM PhD students organise DCASE challenge task on computational bioacoustics","author":"Emmanouil Benetos","date":"Wed 29 May 2024"},"html":"<p>Centre for Digital Music PhD students <a href=\"https://scholar.google.com/citations?user=C1jftogAAAAJ&#x26;hl=en\">Ines Nolasco</a>, <a href=\"https://shubhrsingh22.github.io/\">Shubhr Singh</a>, and <a href=\"https://jinhualiang.github.io/\">Jinhua Liang</a> are organising the task on <a href=\"https://dcase.community/challenge2024/task-few-shot-bioacoustic-event-detection\">Few-shot Bioacoustic Event Detection</a> as part of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE 2024).</p>\n<p>This task addresses a real need from animal researchers by providing a well-defined, constrained yet highly variable domain to evaluate machine learning methodology. It aims to advance the study of audio signal processing and deep learning in the low-resource scenario, particularly in domain adaptation and few-shot learning. Datasets will be released on 1st June 2024, with the challenge deadline being on 15 June 2024.</p>\n<p>Can you build a system that detects an animal sound with only 5 examples?  Let's liaise to push the boundary of computational bioacoustics and machine listening!</p>","id":"cfaa6a83-ed54-520e-8c11-4732a55fab01"},{"fields":{"slug":"/news/2024-05-17.Singing-researchers_uncover_cross-cultural_patterns_in_music_and_language"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e7707bae8a141506c8f08e0d016dab54/2497f/ScienceAdvances.jpg","srcSet":"/static/e7707bae8a141506c8f08e0d016dab54/248cc/ScienceAdvances.jpg 313w,\n/static/e7707bae8a141506c8f08e0d016dab54/8414e/ScienceAdvances.jpg 625w,\n/static/e7707bae8a141506c8f08e0d016dab54/2497f/ScienceAdvances.jpg 1250w","sizes":"(min-width: 1250px) 1250px, 100vw"},"sources":[{"srcSet":"/static/e7707bae8a141506c8f08e0d016dab54/f7ab9/ScienceAdvances.webp 313w,\n/static/e7707bae8a141506c8f08e0d016dab54/c3c5b/ScienceAdvances.webp 625w,\n/static/e7707bae8a141506c8f08e0d016dab54/03ce1/ScienceAdvances.webp 1250w","type":"image/webp","sizes":"(min-width: 1250px) 1250px, 100vw"}]},"width":1250,"height":1250}}},"title":"Singing researchers uncover cross-cultural patterns in music and language ","author":"admin","date":"Fri 17 May 2024"},"html":"<p><b>Predictable melodies in songs may aid social bonding and group synchronisation, according to researchers.</b></p>\n<p>Key points:</p>\n<ul>\n<li>Over 75 researchers from 46 countries participated in the study, singing traditional songs and speaking in their native languages.</li>\n<li>The study found that songs tend to have slower rhythms and higher pitches than speech, suggesting that these features may facilitate synchronisation and social bonding.</li>\n<li>The study provides valuable insights into the evolution of music and language, and has implications for our understanding of human communication.</li>\n</ul>\n<p>A <a href=\"https://www.science.org/doi/10.1126/sciadv.adm9797\">groundbreaking study published in Science Advances</a> has uncovered fascinating cross-cultural patterns in music and language, with Queen Mary University of London's <a href=\"https://www.qmul.ac.uk/eecs/people/profiles/benetosemmanouil.html\">Dr Emmanouil Benetos</a>, Reader in Machine Listening at the School of Electronic Engineering and Computer Science, contributing to the research.</p>\n<p>The study, involving 75 researchers from 46 countries, examined the rhythms, pitches, and timbres of traditional music and speech from a diverse range of cultures. The findings revealed striking similarities in the way music and speech are structured across different languages, suggesting that these patterns may have deep evolutionary roots.</p>\n<p>Dr Benetos, an expert in machine listening and music technology, contributed his expertise in analysing the acoustic features of the recorded music and speech samples. His insights were instrumental in identifying the subtle patterns that emerged from the vast dataset.</p>\n<p>\"This study is a remarkable achievement that brings together researchers from a wide range of disciplines to explore the fundamental connections between music and language,\" commented Dr Benetos. \"The findings have the potential to shed new light on the evolution of human communication and the role of music in our social and cultural lives.\"</p>\n<p>The research also highlighted the importance of diversity in scientific research. By including participants from a wide range of cultural backgrounds, the study captured a broader and more representative picture of human musical and linguistic behavior.</p>\n<p>\"This study is a testament to the power of collaboration and the importance of diversity in scientific research,\" said Dr Benetos. \"By bringing together researchers from different parts of the world, we were able to gain a deeper understanding of these universal patterns in music and language.\"</p>\n<p>The findings of this study have significant implications for our understanding of music, language, and human communication. They also pave the way for future research into the evolutionary origins of music and the role of technology in preserving and analysing cultural traditions.</p>\n<p><b>News coverage</b> on the article includes:</p>\n<ul>\n<li>New York Times: <a href=\"https://www.nytimes.com/2024/05/15/science/universal-music-evolution.html\">https://www.nytimes.com/2024/05/15/science/universal-music-evolution.html</a></li>\n<li>Scientific American: <a href=\"https://www.scientificamerican.com/article/why-do-we-sing-new-analysis-of-folk-songs-finds-similarities-around-the/\">https://www.scientificamerican.com/article/why-do-we-sing-new-analysis-of-folk-songs-finds-similarities-around-the/</a></li>\n<li>Forbes: <a href=\"https://www.forbes.com/sites/evaamsen/2024/05/15/the-complex-connection-between-music-and-language-around-the-world/?sh=7d0fdc607cc7\">https://www.forbes.com/sites/evaamsen/2024/05/15/the-complex-connection-between-music-and-language-around-the-world/?sh=7d0fdc607cc7</a></li>\n</ul>","id":"e60b1ad4-5c07-51e0-be89-c1780e7e6554"},{"fields":{"slug":"/news/2024-05-16.C4DM-PhD_student_gives_talk_at_IoA"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#184868","images":{"fallback":{"src":"/static/3cca2e68957df9b5c50f060357dffd49/b7804/ioa-logo.png","srcSet":"/static/3cca2e68957df9b5c50f060357dffd49/5aead/ioa-logo.png 200w,\n/static/3cca2e68957df9b5c50f060357dffd49/d6138/ioa-logo.png 400w,\n/static/3cca2e68957df9b5c50f060357dffd49/b7804/ioa-logo.png 800w","sizes":"(min-width: 800px) 800px, 100vw"},"sources":[{"srcSet":"/static/3cca2e68957df9b5c50f060357dffd49/2e34e/ioa-logo.webp 200w,\n/static/3cca2e68957df9b5c50f060357dffd49/416c3/ioa-logo.webp 400w,\n/static/3cca2e68957df9b5c50f060357dffd49/c1587/ioa-logo.webp 800w","type":"image/webp","sizes":"(min-width: 800px) 800px, 100vw"}]},"width":800,"height":800}}},"title":"C4DM PhD student to present at the UK's Institute of Acoustics","author":"Emmanouil Benetos","date":"Thu 16 May 2024"},"html":"<p>Centre for Digital Music PhD student <a href=\"https://jinhualiang.github.io/\">Jinhua Liang</a> will be giving a talk on \"Machine listening with natural language\" as part of the UK's Institute of Acoustics event on <a href=\"https://www.ioa.org.uk/civicrm/event/info?reset=1&#x26;id=860\">Artificial Intelligence for Acoustics</a>, taking place on 22 May 2024 at London South Bank University.</p>\n<p>Given the recent increase in use of Artificial Intelligence (AI) tools such as ChatGPT, there has been much speculation and interest in how AI can be harnessed as a tool across the acoustics industry. However, whilst there are many possibilities for application, there are also many questions still to be answered. Jinhua's work will be showcasing <a href=\"https://arxiv.org/abs/2312.00249\">how large language models (LLMs) can be empowered with audition capabilities</a> and <a href=\"https://arxiv.org/pdf/2403.09527\">how LLMs can support audio editing and generation</a>.</p>","id":"ab2d727f-f180-5264-b7b5-2d376a7b7846"},{"fields":{"slug":"/news/2024-05-03.C4DM-hosts_UKAN_SIG-SAIA_meet-up"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/ad3841c0405ecab7fa3b1102bacd8876/b620c/UKAN.jpg","srcSet":"/static/ad3841c0405ecab7fa3b1102bacd8876/70105/UKAN.jpg 354w,\n/static/ad3841c0405ecab7fa3b1102bacd8876/b1e9f/UKAN.jpg 709w,\n/static/ad3841c0405ecab7fa3b1102bacd8876/b620c/UKAN.jpg 1417w","sizes":"(min-width: 1417px) 1417px, 100vw"},"sources":[{"srcSet":"/static/ad3841c0405ecab7fa3b1102bacd8876/ba32b/UKAN.webp 354w,\n/static/ad3841c0405ecab7fa3b1102bacd8876/03116/UKAN.webp 709w,\n/static/ad3841c0405ecab7fa3b1102bacd8876/3e99f/UKAN.webp 1417w","type":"image/webp","sizes":"(min-width: 1417px) 1417px, 100vw"}]},"width":1417,"height":1417}}},"title":"C4DM hosts UK Acoustics Network meetup on Spatial Acoustics and Immersive Audio","author":"Emmanouil Benetos","date":"Fri 03 May 2024"},"html":"<p>The Centre for Digital Music will be hosting the second in-person meet-up of the <a href=\"https://acoustics.ac.uk/sigs/spatial-acoustics-and-immersive-audio/\">UK Acoustics Network special interest group on Spatial Acoustics and Immersive Audio (SIG-SAIA)</a> on the 10th of May, 2024, at Queen Mary University of London. The meetup will be organised by <a href=\"https://www.qmul.ac.uk/eecs/people/profiles/aidanhogg.html\">Dr Aidan Hogg</a>, Lecturer in Computer Science at C4DM and Queen Mary, who is also SIG-SAIA group co-ordinator.</p>\n<p>The meetup will feature short talks from researchers working in the group's remit, including C4DM PhD student <a href=\"https://yoyololicon.github.io/\">Chin-Yun Yu</a> who will give a talk on \"Time-of-arrival estimation and phase unwrapping of head-related transfer functions with integer linear programming\", and C4DM Post-Doctoral Research Assistant <a href=\"https://www.qmul.ac.uk/eecs/people/profiles/sarkarsaurjya.html\">Saurjya Sarkar</a> who will give a talk on \"Time-domain music source separation for choirs and ensembles\". The meetup will conclude with a backstage visit at the Royal Opera House.</p>\n<p>More information on the meetup and information on how to attend can be found at: <a href=\"https://acoustics.ac.uk/second-in-person-sig-saia-meet-up/\">https://acoustics.ac.uk/second-in-person-sig-saia-meet-up/</a></p>","id":"06953097-0021-5655-ab1a-a1c39bdfd930"},{"fields":{"slug":"/news/2024-05-02.AI-in-Music-partnerships"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/e5b2b7867ab8a8f8e145587e58e8baf5/30f07/Music640.jpg","srcSet":"/static/e5b2b7867ab8a8f8e145587e58e8baf5/41624/Music640.jpg 160w,\n/static/e5b2b7867ab8a8f8e145587e58e8baf5/1b894/Music640.jpg 320w,\n/static/e5b2b7867ab8a8f8e145587e58e8baf5/30f07/Music640.jpg 640w","sizes":"(min-width: 640px) 640px, 100vw"},"sources":[{"srcSet":"/static/e5b2b7867ab8a8f8e145587e58e8baf5/60b4d/Music640.webp 160w,\n/static/e5b2b7867ab8a8f8e145587e58e8baf5/5e011/Music640.webp 320w,\n/static/e5b2b7867ab8a8f8e145587e58e8baf5/90d07/Music640.webp 640w","type":"image/webp","sizes":"(min-width: 640px) 640px, 100vw"}]},"width":640,"height":640}}},"title":"AI in Music: Queen Mary begins new research partnerships","author":"Emmanouil Benetos","date":"Thu 02 May 2024"},"html":"<p>The Centre for Digital Music of Queen Mary University of London is starting four new collaborative projects to develop new ways to use AI in music. The partnership projects include working with:</p>\n<ul>\n<li>AudioStrip to retrieve component instruments</li>\n<li>RoEx to replicate professional mixing</li>\n<li>Session to develop an AI co-creator</li>\n<li>Stage and Session to ensure artists are fairly paid for their creativity</li>\n</ul>\n<p>See the full news story in the main QMUL website: <a href=\"https://www.qmul.ac.uk/media/news/2024/se/ai-in-music-queen-mary-begins-new-research-partnerships-.html\">https://www.qmul.ac.uk/media/news/2024/se/ai-in-music-queen-mary-begins-new-research-partnerships-.html</a></p>","id":"fb8f6365-a975-55ae-90f9-9a657900c3b9"},{"fields":{"slug":"/news/2024-04-26.C4DM-at_ICLR_2024"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e1b6924083744f1e06cfc014164defcc/928b9/ICLR2024.png","srcSet":"/static/e1b6924083744f1e06cfc014164defcc/b0268/ICLR2024.png 118w,\n/static/e1b6924083744f1e06cfc014164defcc/f1af1/ICLR2024.png 236w,\n/static/e1b6924083744f1e06cfc014164defcc/928b9/ICLR2024.png 472w","sizes":"(min-width: 472px) 472px, 100vw"},"sources":[{"srcSet":"/static/e1b6924083744f1e06cfc014164defcc/2c82d/ICLR2024.webp 118w,\n/static/e1b6924083744f1e06cfc014164defcc/9bbe7/ICLR2024.webp 236w,\n/static/e1b6924083744f1e06cfc014164defcc/b4137/ICLR2024.webp 472w","type":"image/webp","sizes":"(min-width: 472px) 472px, 100vw"}]},"width":472,"height":472}}},"title":"C4DM at ICLR 2024","author":"Emmanouil Benetos","date":"Fri 26 Apr 2024"},"html":"<p>On 7-11 May, C4DM researchers will participate at the <b><a href=\"https://iclr.cc/\">Twelfth International Conference on Learning Representations (ICLR 2024)</a></b>, taking place in Vienna, Austria. ICLR is the premier gathering of professionals dedicated to the advancement of the branch of artificial intelligence called representation learning, but generally referred to as deep learning.</p>\n<p>C4DM members will be presenting the following paper at the main track of ICLR 2024:</p>\n<ul>\n<li><a href=\"https://iclr.cc/virtual/2024/poster/17510\">MERT: acoustic music understanding model with large-scale self-supervised training</a>, by Yizhi Li, Ruibin Yuan, Ge Zhang, Yinghao MA, Xingran Chen, Hanzhi Yin, Chenghao Xiao, Chenghua Lin, Anton Ragni, Emmanouil Benetos, Norbert Gyenge, Roger Dannenberg, Ruibo Liu, Wenhu Chen, Gus Xia, Yemin Shi, Wenhao Huang, zili wang, Yike Guo, Jie Fu (<a href=\"https://arxiv.org/abs/2306.00107\">postprint</a>)</li>\n</ul>\n<p>C4DM members will also be presenting the following paper at the <a href=\"https://llmagents.github.io/\">ICLR 2024 workshop on LLM Agents</a>:</p>\n<ul>\n<li><a href=\"https://llmagents.github.io/\">WavCraft: Audio Editing and Generation with Large Language Models</a>, by Jinhua Liang, Huan Zhang, Haohe Liu, Yin Cao, Qiuqiang Kong, Xubo Liu, Wenwu Wang, Mark D Plumbley, Huy Phan, Emmanouil Benetos (<a href=\"https://arxiv.org/abs/2403.09527\">postprint</a>)</li>\n</ul>\n<p>See you all at ICLR!</p>","id":"149aeb98-4f52-5d66-9ac0-5a4e2a276d8b"}]}}}