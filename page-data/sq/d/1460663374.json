{"data":{"about":{"html":"<p>The <em><strong>Centre for Digital Music</strong></em> is a world-leading, multidisciplinary research group in the field of music &#x26; audio technology.</p>\n<!-- <p style=\"color: red\">**THIS SEEMS TO BE THE ONLY SUPPORTED WAY OF IMPORTING TWITTER TWEETS IN GATSBY 5 AT THE MOMENT, AFAIK IT MEANS THAT EACH \nTWEET HAS TO BE MANUALLY IMPORTED INTO AN MD FILE...**</p>\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">This is what we do: <a href=\"https://t.co/pkyS6IcIUy\">https://t.co/pkyS6IcIUy</a> - an excellent video intro to the wonderful researchers of <a href=\"https://twitter.com/c4dm?ref_src=twsrc%5Etfw\">@c4dm</a>. <a href=\"https://twitter.com/QMEECS?ref_src=twsrc%5Etfw\">@QMEECS</a> <a href=\"https://twitter.com/QMUL?ref_src=twsrc%5Etfw\">@QMUL</a> <a href=\"https://twitter.com/hashtag/research?src=hash&amp;ref_src=twsrc%5Etfw\">#research</a></p>&mdash; C4DM at QMUL (@c4dm) <a href=\"https://twitter.com/c4dm/status/857989625922695173?ref_src=twsrc%5Etfw\">April 28, 2017</a></blockquote> -->","frontmatter":{"title":"About the centre","video":"https://www.youtube.com/embed/Rcbs4NvMFHM","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png","srcSet":"/static/c138fbce66e709a3f503405435de2f2c/0fc07/c4dm.png 100w,\n/static/c138fbce66e709a3f503405435de2f2c/bc685/c4dm.png 200w,\n/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png 400w,\n/static/c138fbce66e709a3f503405435de2f2c/7ef25/c4dm.png 800w","sizes":"(min-width: 400px) 400px, 100vw"},"sources":[{"srcSet":"/static/c138fbce66e709a3f503405435de2f2c/29e9d/c4dm.webp 100w,\n/static/c138fbce66e709a3f503405435de2f2c/2f9ed/c4dm.webp 200w,\n/static/c138fbce66e709a3f503405435de2f2c/ece87/c4dm.webp 400w,\n/static/c138fbce66e709a3f503405435de2f2c/4bc26/c4dm.webp 800w","type":"image/webp","sizes":"(min-width: 400px) 400px, 100vw"}]},"width":400,"height":60}}}}},"projects":{"nodes":[{"fields":{"slug":"/projects/cdtdata"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080838","images":{"fallback":{"src":"/static/bd2eecf83bf5392a4ac7061919a52914/cd63e/cdtdata.png","srcSet":"/static/bd2eecf83bf5392a4ac7061919a52914/08744/cdtdata.png 145w,\n/static/bd2eecf83bf5392a4ac7061919a52914/3c29b/cdtdata.png 290w,\n/static/bd2eecf83bf5392a4ac7061919a52914/cd63e/cdtdata.png 579w","sizes":"(min-width: 579px) 579px, 100vw"},"sources":[{"srcSet":"/static/bd2eecf83bf5392a4ac7061919a52914/72079/cdtdata.webp 145w,\n/static/bd2eecf83bf5392a4ac7061919a52914/a7f7a/cdtdata.webp 290w,\n/static/bd2eecf83bf5392a4ac7061919a52914/ba6fb/cdtdata.webp 579w","type":"image/webp","sizes":"(min-width: 579px) 579px, 100vw"}]},"width":579,"height":579}}},"title":"Centre for Doctoral Training (CDT) in Data Centric Engineering","author":"Prof Eram Rizvi (PI), Prof Mark Sandler (CI), Prof Nick Bryan-Kinns (CI)","date":null},"html":"","id":"e68ade9f-702f-5695-9a7d-e2ab94af8c72"},{"fields":{"slug":"/projects/Fazekas-UMG2"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Beyond Supervised Learning for Musical Audio","author":"Dr George Fazekas (PI)","date":null},"html":"","id":"433744e4-ecdf-538d-903d-85748cab2ea6"},{"fields":{"slug":"/projects/Fazekas-Sony"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"New Methodologies for Efficient and Controllable Music Generation","author":"Dr George Fazekas (PI)","date":null},"html":"","id":"a3e9f3a1-9edd-50fe-b941-283fe6f7de85"},{"fields":{"slug":"/projects/aimcdt"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/98704461ac8eeac7d8b8a64dc1520bf4/7a590/aimcdt.png","srcSet":"/static/98704461ac8eeac7d8b8a64dc1520bf4/4b686/aimcdt.png 154w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/d2213/aimcdt.png 308w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/7a590/aimcdt.png 615w","sizes":"(min-width: 615px) 615px, 100vw"},"sources":[{"srcSet":"/static/98704461ac8eeac7d8b8a64dc1520bf4/b2942/aimcdt.webp 154w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/46581/aimcdt.webp 308w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/fa942/aimcdt.webp 615w","type":"image/webp","sizes":"(min-width: 615px) 615px, 100vw"}]},"width":615,"height":615}}},"title":"UKRI Centre for Doctoral Training in Artificial Intelligence and Music (AIM)","author":"Prof Simon Dixon (PI), Dr Mathieu Barthet (CI), Dr Nick Bryan-Kinns (CI), Dr Gyorgy Fazekas (CI), Prof Mark Sandler (CI), Dr Andrew McPherson (CI), Dr Emmanouil Benetos (CI)","date":null},"html":"","id":"237af1c8-14dc-5167-a5e1-fa55e6860dea"},{"fields":{"slug":"/projects/Dixon-DAACI"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Performance Rendering for Music Generation Systems","author":"Prof Simon Dixon (PI)","date":null},"html":"","id":"780d9bfc-fda8-5320-aa0c-0a93788ae549"},{"fields":{"slug":"/projects/ZBenetos-Spotify"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Style classification of podcasts using audio","author":"Dr Emmanouil Benetos (PI)","date":null},"html":"","id":"19d1254c-f423-52ab-ae78-51e201f32ab6"}]},"news":{"nodes":[{"fields":{"slug":"/news/2024-06-03.C4DM-PhD_student_predicts_Eurovision_winner"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#b8a898","images":{"fallback":{"src":"/static/234763aa1e5f363371e4bfff769cc48d/70fef/Katarzyna-Adamska.jpg","srcSet":"/static/234763aa1e5f363371e4bfff769cc48d/463c6/Katarzyna-Adamska.jpg 379w,\n/static/234763aa1e5f363371e4bfff769cc48d/53da6/Katarzyna-Adamska.jpg 758w,\n/static/234763aa1e5f363371e4bfff769cc48d/70fef/Katarzyna-Adamska.jpg 1516w","sizes":"(min-width: 1516px) 1516px, 100vw"},"sources":[{"srcSet":"/static/234763aa1e5f363371e4bfff769cc48d/d4721/Katarzyna-Adamska.webp 379w,\n/static/234763aa1e5f363371e4bfff769cc48d/6420f/Katarzyna-Adamska.webp 758w,\n/static/234763aa1e5f363371e4bfff769cc48d/79063/Katarzyna-Adamska.webp 1516w","type":"image/webp","sizes":"(min-width: 1516px) 1516px, 100vw"}]},"width":1516,"height":1516}}},"title":"C4DM PhD student predicts Eurovision winner","author":"Emmanouil Benetos","date":"Mon 03 Jun 2024"},"html":"<p>Using a prediction tool, <a href=\"https://www.qmul.ac.uk/eecs/people/profiles/adamskakatarzynamaria.html\">Katarzyna Adamska</a>, a third year PhD student, correctly predicted that Switzerland’s song entry, ‘The Code’, would win this year’s competition.</p>\n<p>Read the full story in the EECS website: <a href=\"https://www.qmul.ac.uk/eecs/news-and-events/news/items/eecs-phd-researcher-predicts-eurovision-winner-.html\">https://www.qmul.ac.uk/eecs/news-and-events/news/items/eecs-phd-researcher-predicts-eurovision-winner-.html</a></p>","id":"5d4a7783-43de-5e4f-8185-8b86347998fd"},{"fields":{"slug":"/news/2024-06-02.C4DM-PhD_student_gives_talk_at_GDL_workshop"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/ca433a7002ce0a87e1a49ef7a6a0d116/b5658/m4dl.png","srcSet":"/static/ca433a7002ce0a87e1a49ef7a6a0d116/acb7c/m4dl.png 256w,\n/static/ca433a7002ce0a87e1a49ef7a6a0d116/ccc41/m4dl.png 512w,\n/static/ca433a7002ce0a87e1a49ef7a6a0d116/b5658/m4dl.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/ca433a7002ce0a87e1a49ef7a6a0d116/22bfc/m4dl.webp 256w,\n/static/ca433a7002ce0a87e1a49ef7a6a0d116/d689f/m4dl.webp 512w,\n/static/ca433a7002ce0a87e1a49ef7a6a0d116/67ded/m4dl.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"title":"C4DM PhD student to present at the Geometric Deep Learning workshop","author":"Emmanouil Benetos","date":"Sun 02 Jun 2024"},"html":"<p>Centre for Digital Music PhD student <a href=\"https://shubhrsingh22.github.io/\">Shubhr Singh</a> will be presenting his recent work on \"Local-Higher Order GNNs For Audio Classification And Tagging\" as part of the <a href=\"https://maths4dl.ac.uk/newsevents/geometric-deep-learning-workshop-university-of-cambridge-10-12-june-2024\">Geometric Deep Learning workshop</a>, taking place on 10-12 June 2024 at the University of Cambridge. The workshop is organised by the EPSRC's <a href=\"https://maths4dl.ac.uk/\">Mathematics for Deep Learning programme grant (m4DL)</a>.</p>\n<p>Shubhr's work introduces the Local-Higher Order Graph Neural Network (LHGNN), which stands as a significant step forward in the field of audio classification and tagging, providing a robust framework that leverages graph-based and clustering methodologies to achieve high performance.</p>","id":"9df4d5da-9aa9-51b1-9cc8-2e7941da10ad"},{"fields":{"slug":"/news/2024-05-29.C4DM-PhD_students_organise_DCASE_task"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#282828","images":{"fallback":{"src":"/static/1778669a3aa300652155162f4abc6354/0a9b5/DCASE.jpg","srcSet":"/static/1778669a3aa300652155162f4abc6354/384e6/DCASE.jpg 292w,\n/static/1778669a3aa300652155162f4abc6354/75c79/DCASE.jpg 584w,\n/static/1778669a3aa300652155162f4abc6354/0a9b5/DCASE.jpg 1167w","sizes":"(min-width: 1167px) 1167px, 100vw"},"sources":[{"srcSet":"/static/1778669a3aa300652155162f4abc6354/ad81e/DCASE.webp 292w,\n/static/1778669a3aa300652155162f4abc6354/4e6da/DCASE.webp 584w,\n/static/1778669a3aa300652155162f4abc6354/c3035/DCASE.webp 1167w","type":"image/webp","sizes":"(min-width: 1167px) 1167px, 100vw"}]},"width":1167,"height":1167}}},"title":"C4DM PhD students organise DCASE challenge task on computational bioacoustics","author":"Emmanouil Benetos","date":"Wed 29 May 2024"},"html":"<p>Centre for Digital Music PhD students <a href=\"https://scholar.google.com/citations?user=C1jftogAAAAJ&#x26;hl=en\">Ines Nolasco</a>, <a href=\"https://shubhrsingh22.github.io/\">Shubhr Singh</a>, and <a href=\"https://jinhualiang.github.io/\">Jinhua Liang</a> are organising the task on <a href=\"https://dcase.community/challenge2024/task-few-shot-bioacoustic-event-detection\">Few-shot Bioacoustic Event Detection</a> as part of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE 2024).</p>\n<p>This task addresses a real need from animal researchers by providing a well-defined, constrained yet highly variable domain to evaluate machine learning methodology. It aims to advance the study of audio signal processing and deep learning in the low-resource scenario, particularly in domain adaptation and few-shot learning. Datasets will be released on 1st June 2024, with the challenge deadline being on 15 June 2024.</p>\n<p>Can you build a system that detects an animal sound with only 5 examples?  Let's liaise to push the boundary of computational bioacoustics and machine listening!</p>","id":"cfaa6a83-ed54-520e-8c11-4732a55fab01"},{"fields":{"slug":"/news/2024-04-06.C4DM-at_AES_Europe_2024"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/1b4626b83482cd2dbc26206a71263e4a/db4f2/AES-Europe-2024.png","srcSet":"/static/1b4626b83482cd2dbc26206a71263e4a/30cdc/AES-Europe-2024.png 300w,\n/static/1b4626b83482cd2dbc26206a71263e4a/08f46/AES-Europe-2024.png 601w,\n/static/1b4626b83482cd2dbc26206a71263e4a/db4f2/AES-Europe-2024.png 1201w","sizes":"(min-width: 1201px) 1201px, 100vw"},"sources":[{"srcSet":"/static/1b4626b83482cd2dbc26206a71263e4a/078c3/AES-Europe-2024.webp 300w,\n/static/1b4626b83482cd2dbc26206a71263e4a/2b014/AES-Europe-2024.webp 601w,\n/static/1b4626b83482cd2dbc26206a71263e4a/1fcf1/AES-Europe-2024.webp 1201w","type":"image/webp","sizes":"(min-width: 1201px) 1201px, 100vw"}]},"width":1201,"height":1201}}},"title":"C4DM at AES Europe 2024","author":"Ashley Noel-Hirst","date":"Fri 24 May 2024"},"html":"<p>On 15-17 June, C4DM researchers will participate at the <b><a href=\"https://aes2.org/events-calendar/aes-europe-2024/\">Audio Engineering Society's 2024 European Conference (AES Europe 2024)</a></b>. <a href=\"https://aes2.org\">AES</a> is a leading professional body in the field of audio processing. The theme of this year's european conference is \"Echoes of the Past Inspire the Sound of the Future\". All papers will also be published in the Journal of the Audio Engineering Society (JAES).</p>\n<p>The Centre for Digital Music will be represented in the following papers at the AES 2024:</p>\n<ul>\n<li><a href=\"https://arxiv.org/abs/2405.06804\">Time-of-arrival Estimation and Phase Unwrapping of Head-related Transfer Functions With Integer Linear Programming</a>, by Chin-Yun Yu, Johan Pauwels, and György Fazekas. In this paper, the authors solve two tasks for head-related transfer functions: time-of-arrival and phase unwrapping. They evaluate an algorithm which uses graph-based data structure and integer linear programming – achieving state of the art results.</li>\n</ul>\n<!-- * regarding phase unwrapping, they are the first work that perform phase unwrapping in all directions and frequencies at once. -->\n<ul>\n<li><a href=\"https://www.aes.org/e-lib/browse.cfm?elib=22374\">The Role of Communication and Reference Songs in the Mixing Process: Insights From Professional Mix Engineers</a>, by Soumya Sai Vanka, Maryam Safi, Jean-Baptiste Rolland, and György Fazekas. The authors identify how context can be incorporated into design of AI-based user-centric auto mixing tools. This is achieved through semi-structured interviews with professionals followed by a questionnaire-based study which corroborates identified themes.</li>\n</ul>\n<!-- * With a thorough exploration of these themes, they identify key materials enabling understanding of vision for the mix and how it becomes part of mixing engineer's workflow.  -->\n<ul>\n<li><a href=\"https://aeseurope2024.sched.com/event/1dQrO/user-preference-evaluation-of-masking-ratio-in-multiple-speaker-scenarios\">User preference evaluation of Masking Ratio in multiple speaker scenarios</a>, by Xiaojing Liu. This paper presents an experiment exploring user preferences for the Masker-to-Signal Ratio (MSR) in multi-channel content. This is evaluated through a subjective listening test exploring \"preference\" and \"clarity\" in a number of masking conditions.</li>\n</ul>\n<p>You can find the full schedule at: <a href=\"https://aeseurope2024.sched.com\">https://aeseurope2024.sched.com</a></p>\n<p>See you all there!</p>","id":"607580bd-b6ef-57ca-9796-c51e2156ecdd"},{"fields":{"slug":"/news/2024-05-17.Singing-researchers_uncover_cross-cultural_patterns_in_music_and_language"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e7707bae8a141506c8f08e0d016dab54/2497f/ScienceAdvances.jpg","srcSet":"/static/e7707bae8a141506c8f08e0d016dab54/248cc/ScienceAdvances.jpg 313w,\n/static/e7707bae8a141506c8f08e0d016dab54/8414e/ScienceAdvances.jpg 625w,\n/static/e7707bae8a141506c8f08e0d016dab54/2497f/ScienceAdvances.jpg 1250w","sizes":"(min-width: 1250px) 1250px, 100vw"},"sources":[{"srcSet":"/static/e7707bae8a141506c8f08e0d016dab54/f7ab9/ScienceAdvances.webp 313w,\n/static/e7707bae8a141506c8f08e0d016dab54/c3c5b/ScienceAdvances.webp 625w,\n/static/e7707bae8a141506c8f08e0d016dab54/03ce1/ScienceAdvances.webp 1250w","type":"image/webp","sizes":"(min-width: 1250px) 1250px, 100vw"}]},"width":1250,"height":1250}}},"title":"Singing researchers uncover cross-cultural patterns in music and language ","author":"admin","date":"Fri 17 May 2024"},"html":"<p><b>Predictable melodies in songs may aid social bonding and group synchronisation, according to researchers.</b></p>\n<p>Key points:</p>\n<ul>\n<li>Over 75 researchers from 46 countries participated in the study, singing traditional songs and speaking in their native languages.</li>\n<li>The study found that songs tend to have slower rhythms and higher pitches than speech, suggesting that these features may facilitate synchronisation and social bonding.</li>\n<li>The study provides valuable insights into the evolution of music and language, and has implications for our understanding of human communication.</li>\n</ul>\n<p>A <a href=\"https://www.science.org/doi/10.1126/sciadv.adm9797\">groundbreaking study published in Science Advances</a> has uncovered fascinating cross-cultural patterns in music and language, with Queen Mary University of London's <a href=\"https://www.qmul.ac.uk/eecs/people/profiles/benetosemmanouil.html\">Dr Emmanouil Benetos</a>, Reader in Machine Listening at the School of Electronic Engineering and Computer Science, contributing to the research.</p>\n<p>The study, involving 75 researchers from 46 countries, examined the rhythms, pitches, and timbres of traditional music and speech from a diverse range of cultures. The findings revealed striking similarities in the way music and speech are structured across different languages, suggesting that these patterns may have deep evolutionary roots.</p>\n<p>Dr Benetos, an expert in machine listening and music technology, contributed his expertise in analysing the acoustic features of the recorded music and speech samples. His insights were instrumental in identifying the subtle patterns that emerged from the vast dataset.</p>\n<p>\"This study is a remarkable achievement that brings together researchers from a wide range of disciplines to explore the fundamental connections between music and language,\" commented Dr Benetos. \"The findings have the potential to shed new light on the evolution of human communication and the role of music in our social and cultural lives.\"</p>\n<p>The research also highlighted the importance of diversity in scientific research. By including participants from a wide range of cultural backgrounds, the study captured a broader and more representative picture of human musical and linguistic behavior.</p>\n<p>\"This study is a testament to the power of collaboration and the importance of diversity in scientific research,\" said Dr Benetos. \"By bringing together researchers from different parts of the world, we were able to gain a deeper understanding of these universal patterns in music and language.\"</p>\n<p>The findings of this study have significant implications for our understanding of music, language, and human communication. They also pave the way for future research into the evolutionary origins of music and the role of technology in preserving and analysing cultural traditions.</p>\n<p><b>News coverage</b> on the article includes:</p>\n<ul>\n<li>New York Times: <a href=\"https://www.nytimes.com/2024/05/15/science/universal-music-evolution.html\">https://www.nytimes.com/2024/05/15/science/universal-music-evolution.html</a></li>\n<li>Scientific American: <a href=\"https://www.scientificamerican.com/article/why-do-we-sing-new-analysis-of-folk-songs-finds-similarities-around-the/\">https://www.scientificamerican.com/article/why-do-we-sing-new-analysis-of-folk-songs-finds-similarities-around-the/</a></li>\n<li>Forbes: <a href=\"https://www.forbes.com/sites/evaamsen/2024/05/15/the-complex-connection-between-music-and-language-around-the-world/?sh=7d0fdc607cc7\">https://www.forbes.com/sites/evaamsen/2024/05/15/the-complex-connection-between-music-and-language-around-the-world/?sh=7d0fdc607cc7</a></li>\n</ul>","id":"e60b1ad4-5c07-51e0-be89-c1780e7e6554"},{"fields":{"slug":"/news/2024-05-16.C4DM-PhD_student_gives_talk_at_IoA"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#184868","images":{"fallback":{"src":"/static/3cca2e68957df9b5c50f060357dffd49/b7804/ioa-logo.png","srcSet":"/static/3cca2e68957df9b5c50f060357dffd49/5aead/ioa-logo.png 200w,\n/static/3cca2e68957df9b5c50f060357dffd49/d6138/ioa-logo.png 400w,\n/static/3cca2e68957df9b5c50f060357dffd49/b7804/ioa-logo.png 800w","sizes":"(min-width: 800px) 800px, 100vw"},"sources":[{"srcSet":"/static/3cca2e68957df9b5c50f060357dffd49/2e34e/ioa-logo.webp 200w,\n/static/3cca2e68957df9b5c50f060357dffd49/416c3/ioa-logo.webp 400w,\n/static/3cca2e68957df9b5c50f060357dffd49/c1587/ioa-logo.webp 800w","type":"image/webp","sizes":"(min-width: 800px) 800px, 100vw"}]},"width":800,"height":800}}},"title":"C4DM PhD student to present at the UK's Institute of Acoustics","author":"Emmanouil Benetos","date":"Thu 16 May 2024"},"html":"<p>Centre for Digital Music PhD student <a href=\"https://jinhualiang.github.io/\">Jinhua Liang</a> will be giving a talk on \"Machine listening with natural language\" as part of the UK's Institute of Acoustics event on <a href=\"https://www.ioa.org.uk/civicrm/event/info?reset=1&#x26;id=860\">Artificial Intelligence for Acoustics</a>, taking place on 22 May 2024 at London South Bank University.</p>\n<p>Given the recent increase in use of Artificial Intelligence (AI) tools such as ChatGPT, there has been much speculation and interest in how AI can be harnessed as a tool across the acoustics industry. However, whilst there are many possibilities for application, there are also many questions still to be answered. Jinhua's work will be showcasing <a href=\"https://arxiv.org/abs/2312.00249\">how large language models (LLMs) can be empowered with audition capabilities</a> and <a href=\"https://arxiv.org/pdf/2403.09527\">how LLMs can support audio editing and generation</a>.</p>","id":"ab2d727f-f180-5264-b7b5-2d376a7b7846"}]}}}