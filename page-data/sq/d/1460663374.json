{"data":{"about":{"html":"<p>The <em><strong>Centre for Digital Music</strong></em> is a world-leading, multidisciplinary research group in the field of music &#x26; audio technology.</p>\n<!-- <p style=\"color: red\">**THIS SEEMS TO BE THE ONLY SUPPORTED WAY OF IMPORTING TWITTER TWEETS IN GATSBY 5 AT THE MOMENT, AFAIK IT MEANS THAT EACH \nTWEET HAS TO BE MANUALLY IMPORTED INTO AN MD FILE...**</p>\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">This is what we do: <a href=\"https://t.co/pkyS6IcIUy\">https://t.co/pkyS6IcIUy</a> - an excellent video intro to the wonderful researchers of <a href=\"https://twitter.com/c4dm?ref_src=twsrc%5Etfw\">@c4dm</a>. <a href=\"https://twitter.com/QMEECS?ref_src=twsrc%5Etfw\">@QMEECS</a> <a href=\"https://twitter.com/QMUL?ref_src=twsrc%5Etfw\">@QMUL</a> <a href=\"https://twitter.com/hashtag/research?src=hash&amp;ref_src=twsrc%5Etfw\">#research</a></p>&mdash; C4DM at QMUL (@c4dm) <a href=\"https://twitter.com/c4dm/status/857989625922695173?ref_src=twsrc%5Etfw\">April 28, 2017</a></blockquote> -->","frontmatter":{"title":"About the centre","video":"https://www.youtube.com/embed/Rcbs4NvMFHM","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png","srcSet":"/static/c138fbce66e709a3f503405435de2f2c/0fc07/c4dm.png 100w,\n/static/c138fbce66e709a3f503405435de2f2c/bc685/c4dm.png 200w,\n/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png 400w,\n/static/c138fbce66e709a3f503405435de2f2c/7ef25/c4dm.png 800w","sizes":"(min-width: 400px) 400px, 100vw"},"sources":[{"srcSet":"/static/c138fbce66e709a3f503405435de2f2c/29e9d/c4dm.webp 100w,\n/static/c138fbce66e709a3f503405435de2f2c/2f9ed/c4dm.webp 200w,\n/static/c138fbce66e709a3f503405435de2f2c/ece87/c4dm.webp 400w,\n/static/c138fbce66e709a3f503405435de2f2c/4bc26/c4dm.webp 800w","type":"image/webp","sizes":"(min-width: 400px) 400px, 100vw"}]},"width":400,"height":60}}}}},"projects":{"nodes":[{"fields":{"slug":"/projects/cdtdata"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080838","images":{"fallback":{"src":"/static/bd2eecf83bf5392a4ac7061919a52914/cd63e/cdtdata.png","srcSet":"/static/bd2eecf83bf5392a4ac7061919a52914/08744/cdtdata.png 145w,\n/static/bd2eecf83bf5392a4ac7061919a52914/3c29b/cdtdata.png 290w,\n/static/bd2eecf83bf5392a4ac7061919a52914/cd63e/cdtdata.png 579w","sizes":"(min-width: 579px) 579px, 100vw"},"sources":[{"srcSet":"/static/bd2eecf83bf5392a4ac7061919a52914/72079/cdtdata.webp 145w,\n/static/bd2eecf83bf5392a4ac7061919a52914/a7f7a/cdtdata.webp 290w,\n/static/bd2eecf83bf5392a4ac7061919a52914/ba6fb/cdtdata.webp 579w","type":"image/webp","sizes":"(min-width: 579px) 579px, 100vw"}]},"width":579,"height":579}}},"title":"Centre for Doctoral Training (CDT) in Data Centric Engineering","author":"Prof Eram Rizvi (PI), Prof Mark Sandler (CI), Prof Nick Bryan-Kinns (CI)","date":null},"html":"","id":"e68ade9f-702f-5695-9a7d-e2ab94af8c72"},{"fields":{"slug":"/projects/Fazekas-Sony"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"New Methodologies for Efficient and Controllable Music Generation","author":"Dr George Fazekas (PI)","date":null},"html":"","id":"a3e9f3a1-9edd-50fe-b941-283fe6f7de85"},{"fields":{"slug":"/projects/Fazekas-UMG2"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Beyond Supervised Learning for Musical Audio","author":"Dr George Fazekas (PI)","date":null},"html":"","id":"433744e4-ecdf-538d-903d-85748cab2ea6"},{"fields":{"slug":"/projects/aimcdt"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/98704461ac8eeac7d8b8a64dc1520bf4/7a590/aimcdt.png","srcSet":"/static/98704461ac8eeac7d8b8a64dc1520bf4/4b686/aimcdt.png 154w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/d2213/aimcdt.png 308w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/7a590/aimcdt.png 615w","sizes":"(min-width: 615px) 615px, 100vw"},"sources":[{"srcSet":"/static/98704461ac8eeac7d8b8a64dc1520bf4/b2942/aimcdt.webp 154w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/46581/aimcdt.webp 308w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/fa942/aimcdt.webp 615w","type":"image/webp","sizes":"(min-width: 615px) 615px, 100vw"}]},"width":615,"height":615}}},"title":"UKRI Centre for Doctoral Training in Artificial Intelligence and Music (AIM)","author":"Prof Simon Dixon (PI), Dr Mathieu Barthet (CI), Dr Nick Bryan-Kinns (CI), Dr Gyorgy Fazekas (CI), Prof Mark Sandler (CI), Dr Andrew McPherson (CI), Dr Emmanouil Benetos (CI)","date":null},"html":"","id":"237af1c8-14dc-5167-a5e1-fa55e6860dea"},{"fields":{"slug":"/projects/Dixon-DAACI"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Performance Rendering for Music Generation Systems","author":"Prof Simon Dixon (PI)","date":null},"html":"","id":"780d9bfc-fda8-5320-aa0c-0a93788ae549"},{"fields":{"slug":"/projects/ZBenetos-Spotify"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Style classification of podcasts using audio","author":"Dr Emmanouil Benetos (PI)","date":null},"html":"","id":"19d1254c-f423-52ab-ae78-51e201f32ab6"}]},"news":{"nodes":[{"fields":{"slug":"/news/2024-05-03.C4DM-hosts_UKAN_SIG-SAIA_meet-up"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/a489b9628df16e3652fd223e7c53d501/b765b/UKAN.png","srcSet":"/static/a489b9628df16e3652fd223e7c53d501/e8954/UKAN.png 97w,\n/static/a489b9628df16e3652fd223e7c53d501/b8b53/UKAN.png 194w,\n/static/a489b9628df16e3652fd223e7c53d501/b765b/UKAN.png 387w","sizes":"(min-width: 387px) 387px, 100vw"},"sources":[{"srcSet":"/static/a489b9628df16e3652fd223e7c53d501/fdab6/UKAN.webp 97w,\n/static/a489b9628df16e3652fd223e7c53d501/22d94/UKAN.webp 194w,\n/static/a489b9628df16e3652fd223e7c53d501/9116b/UKAN.webp 387w","type":"image/webp","sizes":"(min-width: 387px) 387px, 100vw"}]},"width":387,"height":387}}},"title":"C4DM hosts UK Acoustics Network meetup on Spatial Acoustics and Immersive Audio","author":"Emmanouil Benetos","date":"Fri 03 May 2024"},"html":"<p>The Centre for Digital Music will be hosting the second in-person meet-up of the <a href=\"https://acoustics.ac.uk/sigs/spatial-acoustics-and-immersive-audio/\">UK Acoustics Network special interest group on Spatial Acoustics and Immersive Audio (SIG-SAIA)</a> on the 10th of May, 2024, at Queen Mary University of London. The meetup will be organised by <a href=\"https://www.qmul.ac.uk/eecs/people/profiles/aidanhogg.html\">Dr Aidan Hogg</a>, Lecturer in Computer Science at C4DM and Queen Mary, who is also SIG-SAIA group co-ordinator.</p>\n<p>The meetup will feature short talks from researchers working in the group's remit, including C4DM PhD student <a href=\"https://yoyololicon.github.io/\">Chin-Yun Yu</a> who will give a talk on \"Time-of-arrival estimation and phase unwrapping of head-related transfer functions with integer linear programming\", and C4DM Post-Doctoral Research Assistant <a href=\"https://www.qmul.ac.uk/eecs/people/profiles/sarkarsaurjya.html\">Saurjya Sarkar</a> who will give a talk on \"Time-domain music source separation for choirs and ensembles\". The meetup will conclude with a backstage visit at the Royal Opera House.</p>\n<p>More information on the meetup and information on how to attend can be found at: <a href=\"https://acoustics.ac.uk/second-in-person-sig-saia-meet-up/\">https://acoustics.ac.uk/second-in-person-sig-saia-meet-up/</a></p>","id":"06953097-0021-5655-ab1a-a1c39bdfd930"},{"fields":{"slug":"/news/2024-05-02.AI-in-Music-partnerships"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/e5b2b7867ab8a8f8e145587e58e8baf5/30f07/Music640.jpg","srcSet":"/static/e5b2b7867ab8a8f8e145587e58e8baf5/41624/Music640.jpg 160w,\n/static/e5b2b7867ab8a8f8e145587e58e8baf5/1b894/Music640.jpg 320w,\n/static/e5b2b7867ab8a8f8e145587e58e8baf5/30f07/Music640.jpg 640w","sizes":"(min-width: 640px) 640px, 100vw"},"sources":[{"srcSet":"/static/e5b2b7867ab8a8f8e145587e58e8baf5/60b4d/Music640.webp 160w,\n/static/e5b2b7867ab8a8f8e145587e58e8baf5/5e011/Music640.webp 320w,\n/static/e5b2b7867ab8a8f8e145587e58e8baf5/90d07/Music640.webp 640w","type":"image/webp","sizes":"(min-width: 640px) 640px, 100vw"}]},"width":640,"height":640}}},"title":"AI in Music: Queen Mary begins new research partnerships","author":"Emmanouil Benetos","date":"Thu 02 May 2024"},"html":"<p>The Centre for Digital Music of Queen Mary University of London is starting four new collaborative projects to develop new ways to use AI in music. The partnership projects include working with:</p>\n<ul>\n<li>AudioStrip to retrieve component instruments</li>\n<li>RoEx to replicate professional mixing</li>\n<li>Session to develop an AI co-creator</li>\n<li>Stage and Session to ensure artists are fairly paid for their creativity</li>\n</ul>\n<p>See the full news story in the main QMUL website: <a href=\"https://www.qmul.ac.uk/media/news/2024/se/ai-in-music-queen-mary-begins-new-research-partnerships-.html\">https://www.qmul.ac.uk/media/news/2024/se/ai-in-music-queen-mary-begins-new-research-partnerships-.html</a></p>","id":"fb8f6365-a975-55ae-90f9-9a657900c3b9"},{"fields":{"slug":"/news/2024-04-26.C4DM-at_ICLR_2024"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/e1b6924083744f1e06cfc014164defcc/928b9/ICLR2024.png","srcSet":"/static/e1b6924083744f1e06cfc014164defcc/b0268/ICLR2024.png 118w,\n/static/e1b6924083744f1e06cfc014164defcc/f1af1/ICLR2024.png 236w,\n/static/e1b6924083744f1e06cfc014164defcc/928b9/ICLR2024.png 472w","sizes":"(min-width: 472px) 472px, 100vw"},"sources":[{"srcSet":"/static/e1b6924083744f1e06cfc014164defcc/2c82d/ICLR2024.webp 118w,\n/static/e1b6924083744f1e06cfc014164defcc/9bbe7/ICLR2024.webp 236w,\n/static/e1b6924083744f1e06cfc014164defcc/b4137/ICLR2024.webp 472w","type":"image/webp","sizes":"(min-width: 472px) 472px, 100vw"}]},"width":472,"height":472}}},"title":"C4DM at ICLR 2024","author":"Emmanouil Benetos","date":"Fri 26 Apr 2024"},"html":"<p>On 7-11 May, C4DM researchers will participate at the <b><a href=\"https://iclr.cc/\">Twelfth International Conference on Learning Representations (ICLR 2024)</a></b>, taking place in Vienna, Austria. ICLR is the premier gathering of professionals dedicated to the advancement of the branch of artificial intelligence called representation learning, but generally referred to as deep learning.</p>\n<p>C4DM members will be presenting the following paper at the main track of ICLR 2024:</p>\n<ul>\n<li><a href=\"https://iclr.cc/virtual/2024/poster/17510\">MERT: acoustic music understanding model with large-scale self-supervised training</a>, by Yizhi Li, Ruibin Yuan, Ge Zhang, Yinghao MA, Xingran Chen, Hanzhi Yin, Chenghao Xiao, Chenghua Lin, Anton Ragni, Emmanouil Benetos, Norbert Gyenge, Roger Dannenberg, Ruibo Liu, Wenhu Chen, Gus Xia, Yemin Shi, Wenhao Huang, zili wang, Yike Guo, Jie Fu (<a href=\"https://arxiv.org/abs/2306.00107\">postprint</a>)</li>\n</ul>\n<p>C4DM members will also be presenting the following paper at the <a href=\"https://llmagents.github.io/\">ICLR 2024 workshop on LLM Agents</a>:</p>\n<ul>\n<li><a href=\"https://llmagents.github.io/\">WavCraft: Audio Editing and Generation with Large Language Models</a>, by Jinhua Liang, Huan Zhang, Haohe Liu, Yin Cao, Qiuqiang Kong, Xubo Liu, Wenwu Wang, Mark D Plumbley, Huy Phan, Emmanouil Benetos (<a href=\"https://arxiv.org/abs/2403.09527\">postprint</a>)</li>\n</ul>\n<p>See you all at ICLR!</p>","id":"149aeb98-4f52-5d66-9ac0-5a4e2a276d8b"},{"fields":{"slug":"/news/2024-04-06.C4DM-at_ICASSP_2024"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/8407702d136537b5f2f75c5a80fedd21/47930/ICASSP2024.jpg","srcSet":"/static/8407702d136537b5f2f75c5a80fedd21/e07e1/ICASSP2024.jpg 100w,\n/static/8407702d136537b5f2f75c5a80fedd21/dd515/ICASSP2024.jpg 200w,\n/static/8407702d136537b5f2f75c5a80fedd21/47930/ICASSP2024.jpg 400w","sizes":"(min-width: 400px) 400px, 100vw"},"sources":[{"srcSet":"/static/8407702d136537b5f2f75c5a80fedd21/d8057/ICASSP2024.webp 100w,\n/static/8407702d136537b5f2f75c5a80fedd21/2e34e/ICASSP2024.webp 200w,\n/static/8407702d136537b5f2f75c5a80fedd21/416c3/ICASSP2024.webp 400w","type":"image/webp","sizes":"(min-width: 400px) 400px, 100vw"}]},"width":400,"height":400}}},"title":"C4DM at ICASSP 2024","author":"Emmanouil Benetos","date":"Sat 06 Apr 2024"},"html":"<p>On 14-19 April, several C4DM researchers will participate at the <b><a href=\"https://2024.ieeeicassp.org/\">2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2024)</a></b>. ICASSP is the leading conference in the field of signal processing and the flagship event of the <a href=\"https://signalprocessingsociety.org/\">IEEE Signal Processing Society</a>.</p>\n<p>As in previous years, the Centre for Digital Music will have a strong presence at the conference, both in terms of numbers and overall impact. The below papers presented at ICASSP 2024 are authored or co-authored by C4DM members:</p>\n<ul>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/10446182\">High resolution guitar transcription via domain adaptation</a>, by Xavier Riley, Drew Edwards, and Simon Dixon</p>\n</li>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/10446908\">Learning from taxonomy: multi-label few-shot classification for everyday sound recognition</a>, by Jinhua Liang, Huy Phan, and Emmanouil Benetos</p>\n</li>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/10446400\">Bass accompaniment generation via latent diffusion</a>, by Marco Pasini, Maarten Grachten, and Stefan Lattner</p>\n</li>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/10447564\">Unsupervised pitch-timbre disentanglement of musical instruments using a Jacobian disentangled sequential autoencoder</a>, by Yin-Jyun Luo, Sebastian Ewert, and Simon Dixon</p>\n</li>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/10447835\">Posterior variance-parameterised Gaussian dropout: improving disentangled sequential autoencoders for zero-shot voice conversion</a>, by Yin-Jyun Luo and Simon Dixon</p>\n</li>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/10447445\">Mertech: instrument playing technique detection using self-supervised pretrained model with multi-task finetuning</a>, by Dichucheng Li, Yinghao Ma, Weixing Wei, Qiuqiang Kong, Yulun Wu, Mingjin Che, Fan Xia, Emmanouil Benetos, and Wei Li</p>\n</li>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/10447122\">Generalized multi-source inference for text conditioned music diffusion models</a>, by Emilian Postolache, Giorgio Mariani, Luca Cosmo, Emmanouil Benetos, and Emanuele Rodolà</p>\n</li>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/10447063\">Syncfusion: Multimodal Onset-Synchronized Video-to-Audio Foley Synthesis</a>, by Marco Comunità, Riccardo F. Gramaccioni, Emilian Postolache, Emanuele Rodolà, Danilo Comminiello, and Joshua D. Reiss</p>\n</li>\n</ul>\n<p>The following paper will be presented at the <a href=\"https://xai-sa-workshop.github.io/web/Home.html\">ICASSP Workshop on Explainable Machine Learning for Speech and Audio</a>:</p>\n<ul>\n<li><a href=\"https://www.researchgate.net/publication/379085262_Explainable_Modeling_of_Gender-Targeting_Practices_in_Toy_Advertising_Sound_and_Music\">Explainable modeling of gender-targeting practices in toy advertising sound and music</a>, by Luca Marinelli and Charalampos Saitis</li>\n</ul>\n<p>See you all at ICASSP!</p>","id":"b75a7542-b72f-56b1-9e20-2bcf429b1e64"},{"fields":{"slug":"/news/2023-05-26.C4DM-at_ICMPC_2023"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/2b6f0fee66cf2e8e3786de08167a40f2/ca426/ICMPC2023.png","srcSet":"/static/2b6f0fee66cf2e8e3786de08167a40f2/bb5d0/ICMPC2023.png 126w,\n/static/2b6f0fee66cf2e8e3786de08167a40f2/2cc56/ICMPC2023.png 253w,\n/static/2b6f0fee66cf2e8e3786de08167a40f2/ca426/ICMPC2023.png 505w","sizes":"(min-width: 505px) 505px, 100vw"},"sources":[{"srcSet":"/static/2b6f0fee66cf2e8e3786de08167a40f2/fabbe/ICMPC2023.webp 126w,\n/static/2b6f0fee66cf2e8e3786de08167a40f2/2d900/ICMPC2023.webp 253w,\n/static/2b6f0fee66cf2e8e3786de08167a40f2/0b3af/ICMPC2023.webp 505w","type":"image/webp","sizes":"(min-width: 505px) 505px, 100vw"}]},"width":505,"height":505}}},"title":"C4DM at ICMPC 2023","author":"Admin","date":"Fri 26 May 2023"},"html":"<p>From 24 to 28 August, several C4DM researchers will participate in the <b><a href=\"https://jsmpc.org/ICMPC17/\">17th International Conference on Music Perception and Cognition (ICMPC 2023)</a></b>.</p>\n<p>Below is a list of all the contributions authored or co-authored by C4DM members:</p>\n<ul>\n<li>\n<p><strong>Analysing the Gendering of Music in Toy Commercials via Mid-level Perceptual Features</strong> by Luca Marinelli, and Charalampos Saitis</p>\n</li>\n<li>\n<p><strong>Exploring the Role of Audio and Lyrics in Explaining Moral Worldviews</strong> by Vjosa Preniqi, Kyriaki Kalimeri, and Charalampos Saitis</p>\n</li>\n<li>\n<p><strong>Evolution of Moral Valence in Lyrics Over Time</strong> by Vjosa Preniqi, Kyriaki Kalimeri, Andreas Kaltenbrunner, and Charalampos Saitis</p>\n</li>\n<li>\n<p><strong>Modelling the perception of large-scale order in music</strong> by Edward Hall and Marcus Pearce</p>\n</li>\n<li>\n<p><strong>The Billboard Melodic Music Dataset and Trajectories in Western Pop Music</strong> by Madeline Hamilton and Marcus Pearce</p>\n</li>\n<li>\n<p><strong>Informational constraints and trade-offs in melodies across cultures</strong> by John McBride, Marcus Pearce and Tsvi Tlusty</p>\n</li>\n<li>\n<p><strong>The Cultural Distance Hypothesis and its Impact on Melodic Perception</strong> by Mathias Klarlund, Elvira Brattico, Yi Du, Marcus Pearce, Peter Vuust, Morten Overgaard and Yiyang Wu</p>\n</li>\n<li>\n<p><strong>Predictive processes shape the relation between groove and syncopation</strong> by Tomas Matthews, Jonathan Cannon, Victor Pando-Naude, Jan Stupacher, Isaac Romkey, Thomas Kaplan, Gunvor Bertelsen, Alexandre Celma Miralles, Virginia Penhune and Peter Vuust</p>\n</li>\n</ul>\n<p>The following symposia are co-organised by C4DM members:</p>\n<ul>\n<li>\n<p><strong>Symposium: Predictions in Music: Neurocognitive Insights and Computational Approaches</strong> by Vincent Cheung, Peter Harrison, Psyche Loui, Marcus Pearce and Daniela Sammler</p>\n</li>\n<li>\n<p><strong>Symposium: Modelling rhythm perception beyond the beat</strong> by Atser Damsma, Fleur Bouwer, Lauren Fink, Jonathan Cannon, Keith Doelling, Jessica Grahn, Henkjan Honing and Thomas Kaplan</p>\n</li>\n</ul>\n<p>Happy ICMPCing!</p>","id":"a0eca885-ec21-52d4-bb86-4ee40d56ae4f"},{"fields":{"slug":"/news/2023-05-26.C4DM-at_NIME_2023"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/4eb8fc854adc4eb2ac813efcfae5b7ef/6c0ff/nime2023.png","srcSet":"/static/4eb8fc854adc4eb2ac813efcfae5b7ef/de3a1/nime2023.png 150w,\n/static/4eb8fc854adc4eb2ac813efcfae5b7ef/30cdc/nime2023.png 300w,\n/static/4eb8fc854adc4eb2ac813efcfae5b7ef/6c0ff/nime2023.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/4eb8fc854adc4eb2ac813efcfae5b7ef/c65bc/nime2023.webp 150w,\n/static/4eb8fc854adc4eb2ac813efcfae5b7ef/078c3/nime2023.webp 300w,\n/static/4eb8fc854adc4eb2ac813efcfae5b7ef/3b6e5/nime2023.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"C4DM at NIME 2023","author":"Admin","date":"Fri 26 May 2023"},"html":"<p>From 31 May to 3 June, several C4DM researchers will participate in the <b><a href=\"http://nime2023.org/\">2023 International Conference on New Interfaces for Musical Expression (NIME 2023)</a></b>. Once again, this year's edition will have <b><a href=\"https://bela.io/\">Bela</a></b> as an official sponsor.</p>\n<p>Below is a list of all the publications authored or co-authored by C4DM members:</p>\n<ul>\n<li>\n<p><a href=\"\">Instructions not included: Dementia Friendly approaches to DMI Design</a> by Jon Pigrem, Jennifer Macritchie, and Andrew McPherson</p>\n</li>\n<li>\n<p><a href=\"https://sebastianlobbers.com/static/9324af8a968715544e88586037601b98/SketchSynth_Lobbers_NIME.pdf\">SketchSynth: a browser-based sketching interface for sound control</a> by Sebastian Löbbers and György Fazekas</p>\n</li>\n<li>\n<p><a href=\"https://arxiv.org/abs/2305.14867\">Interactive Neural Resonators</a> by Rodrigo Diaz, Charalampos Saitis, and Mark Sandler</p>\n</li>\n<li>\n<p><a href=\"https://teodannemann.files.wordpress.com/2023/05/nime2023_disability-2.pdf\">Music jamming as a participatory design method. A case study with disabled musicians</a> by Teodoro Dannemann</p>\n</li>\n<li>\n<p><a href=\"https://teodannemann.files.wordpress.com/2023/05/nime2023_wip-5.pdf\">The Sabotaging Piano: key-to-pitch remapping as a source of new techniques in piano improvisation</a> by Teodoro Dannemann and Nick Bryan-Kinns</p>\n</li>\n<li>\n<p><a href=\"https://teodannemann.files.wordpress.com/2023/05/nime2023_final-4.pdf\">Self-Sabotage Workshop: a starting point to unravel sabotaging of instruments as a design practice</a> by Teodoro Dannemann, Nick Bryan-Kinns, Andrew McPherson</p>\n</li>\n<li>\n<p><a href=\"https://teodannemann.files.wordpress.com/2023/05/nime2023_music-1.pdf\">Sabotaging Piano Concert (Music Submission)</a> by Teodoro Dannemann</p>\n</li>\n<li>\n<p><a href=\"\">Exploring the (un)ambiguous guitar: A Qualitative Study on the use of Gesture Disambiguation in Augmented Instrument Design</a> by Adan L. Benito Temprano, Teodoro Dannemann, and Andrew P. McPherson</p>\n</li>\n<li>\n<p><a href=\"https://www.teresapelinski.com/documents/2023-nime-cr-pipeline-nn-bela-v3.pdf\">Pipeline for recording datasets and running neural networks on the Bela embedded hardware platform</a> by Teresa Pelinski, Rodrigo Diaz,  Adan L. Benito Temprano, Andrew McPherson</p>\n</li>\n</ul>\n<p>The following workshop is co-organised by C4DM members:</p>\n<ul>\n<li><a href=\"https://qe4nime.github.io/\">Querying Experience with Musical Interaction Workshop</a> by Courtney N. Reed, Eevee Zayas-Garin, and Andrew McPherson</li>\n</ul>\n<p>Happy NIMEing!</p>","id":"458b680f-64c2-5d74-8883-41b17772c24b"}]}}}