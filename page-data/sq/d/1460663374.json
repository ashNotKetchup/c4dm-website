{"data":{"about":{"html":"<p>The <em><strong>Centre for Digital Music</strong></em> is a world-leading, multidisciplinary research group in the field of music &#x26; audio technology.</p>\n<!-- <p style=\"color: red\">**THIS SEEMS TO BE THE ONLY SUPPORTED WAY OF IMPORTING TWITTER TWEETS IN GATSBY 5 AT THE MOMENT, AFAIK IT MEANS THAT EACH \nTWEET HAS TO BE MANUALLY IMPORTED INTO AN MD FILE...**</p>\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">This is what we do: <a href=\"https://t.co/pkyS6IcIUy\">https://t.co/pkyS6IcIUy</a> - an excellent video intro to the wonderful researchers of <a href=\"https://twitter.com/c4dm?ref_src=twsrc%5Etfw\">@c4dm</a>. <a href=\"https://twitter.com/QMEECS?ref_src=twsrc%5Etfw\">@QMEECS</a> <a href=\"https://twitter.com/QMUL?ref_src=twsrc%5Etfw\">@QMUL</a> <a href=\"https://twitter.com/hashtag/research?src=hash&amp;ref_src=twsrc%5Etfw\">#research</a></p>&mdash; C4DM at QMUL (@c4dm) <a href=\"https://twitter.com/c4dm/status/857989625922695173?ref_src=twsrc%5Etfw\">April 28, 2017</a></blockquote> -->","frontmatter":{"title":"About the centre","video":"https://www.youtube.com/embed/Rcbs4NvMFHM","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png","srcSet":"/static/c138fbce66e709a3f503405435de2f2c/0fc07/c4dm.png 100w,\n/static/c138fbce66e709a3f503405435de2f2c/bc685/c4dm.png 200w,\n/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png 400w,\n/static/c138fbce66e709a3f503405435de2f2c/7ef25/c4dm.png 800w","sizes":"(min-width: 400px) 400px, 100vw"},"sources":[{"srcSet":"/static/c138fbce66e709a3f503405435de2f2c/29e9d/c4dm.webp 100w,\n/static/c138fbce66e709a3f503405435de2f2c/2f9ed/c4dm.webp 200w,\n/static/c138fbce66e709a3f503405435de2f2c/ece87/c4dm.webp 400w,\n/static/c138fbce66e709a3f503405435de2f2c/4bc26/c4dm.webp 800w","type":"image/webp","sizes":"(min-width: 400px) 400px, 100vw"}]},"width":400,"height":60}}}}},"projects":{"nodes":[{"fields":{"slug":"/projects/cdtdata"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080838","images":{"fallback":{"src":"/static/bd2eecf83bf5392a4ac7061919a52914/cd63e/cdtdata.png","srcSet":"/static/bd2eecf83bf5392a4ac7061919a52914/08744/cdtdata.png 145w,\n/static/bd2eecf83bf5392a4ac7061919a52914/3c29b/cdtdata.png 290w,\n/static/bd2eecf83bf5392a4ac7061919a52914/cd63e/cdtdata.png 579w","sizes":"(min-width: 579px) 579px, 100vw"},"sources":[{"srcSet":"/static/bd2eecf83bf5392a4ac7061919a52914/72079/cdtdata.webp 145w,\n/static/bd2eecf83bf5392a4ac7061919a52914/a7f7a/cdtdata.webp 290w,\n/static/bd2eecf83bf5392a4ac7061919a52914/ba6fb/cdtdata.webp 579w","type":"image/webp","sizes":"(min-width: 579px) 579px, 100vw"}]},"width":579,"height":579}}},"title":"Centre for Doctoral Training (CDT) in Data Centric Engineering","author":"Prof Eram Rizvi (PI), Prof Mark Sandler (CI), Prof Nick Bryan-Kinns (CI)","date":null},"html":"","id":"e68ade9f-702f-5695-9a7d-e2ab94af8c72"},{"fields":{"slug":"/projects/Fazekas-Sony"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"New Methodologies for Efficient and Controllable Music Generation","author":"Dr George Fazekas (PI)","date":null},"html":"","id":"a3e9f3a1-9edd-50fe-b941-283fe6f7de85"},{"fields":{"slug":"/projects/Fazekas-UMG2"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Beyond Supervised Learning for Musical Audio","author":"Dr George Fazekas (PI)","date":null},"html":"","id":"433744e4-ecdf-538d-903d-85748cab2ea6"},{"fields":{"slug":"/projects/aimcdt"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/98704461ac8eeac7d8b8a64dc1520bf4/7a590/aimcdt.png","srcSet":"/static/98704461ac8eeac7d8b8a64dc1520bf4/4b686/aimcdt.png 154w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/d2213/aimcdt.png 308w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/7a590/aimcdt.png 615w","sizes":"(min-width: 615px) 615px, 100vw"},"sources":[{"srcSet":"/static/98704461ac8eeac7d8b8a64dc1520bf4/b2942/aimcdt.webp 154w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/46581/aimcdt.webp 308w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/fa942/aimcdt.webp 615w","type":"image/webp","sizes":"(min-width: 615px) 615px, 100vw"}]},"width":615,"height":615}}},"title":"UKRI Centre for Doctoral Training in Artificial Intelligence and Music (AIM)","author":"Prof Simon Dixon (PI), Dr Mathieu Barthet (CI), Dr Nick Bryan-Kinns (CI), Dr Gyorgy Fazekas (CI), Prof Mark Sandler (CI), Dr Andrew McPherson (CI), Dr Emmanouil Benetos (CI)","date":null},"html":"","id":"237af1c8-14dc-5167-a5e1-fa55e6860dea"},{"fields":{"slug":"/projects/Dixon-DAACI"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Performance Rendering for Music Generation Systems","author":"Prof Simon Dixon (PI)","date":null},"html":"","id":"780d9bfc-fda8-5320-aa0c-0a93788ae549"},{"fields":{"slug":"/projects/ZBenetos-Spotify"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Style classification of podcasts using audio","author":"Dr Emmanouil Benetos (PI)","date":null},"html":"","id":"19d1254c-f423-52ab-ae78-51e201f32ab6"}]},"news":{"nodes":[{"fields":{"slug":"/news/news.2024-06-21.C4DM- Ilyass_Moummad"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar:  Ilyass Moummad","author":"Admin","date":"Fri 21 Jun 2024"},"html":"<h3>C4DM Seminar: Ilyass Moummad: Self-Supervised Invariant Learning of Bird Sound Representations</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong><br>\nIlyass Moummad</p>\n<p><strong>Date/time:  Friday, 21st June 2024, 2pm</strong></p>\n<p>**Location: TBA, Greduate Centre, QMUL, E1 4NS **\nZoom: <a href=\"https://qmul-ac-uk.zoom.us/j/9798452959\">https://qmul-ac-uk.zoom.us/j/9798452959</a></p>\n<p><b>Title</b>: Self-Supervised Invariant Learning of Bird Sound Representations</p>\n<hr>\n<p><b>Abstract</b>: Abstract: Self-supervised learning (SSL) involves the learning of data representations without any manual annotation. It consists of solving a pretext task relevant for learning informative data representations, which can be used for transfer learning to solve downstream tasks. Among the different learning paradigms, the most successful in learning discriminative features for classification tasks are “Invariant Learning” methods. They train the model to be insensitive to pre-defined transformations (e.g. if pitch shift is used as a transformation, the model is shown two versions of the same signal with different pitch shifts and is trained to output the same representation for both versions). The choice of data transformations for learning invariance is crucial and depends on the data domain and its relevance to downstream tasks.\nIn bioacoustics, it is not yet clear which data transformations the model should be robust to. In this work, we show that simple and domain-agnostic data augmentations (which do not use any prior knowledge of the nature of bioacoustic sounds) can learn robust and informative features. We evaluate the learned representations through transfer learning to downstream tasks with different challenges such as novel classes (downstream datasets can have classes never seen during pretraining), few-shot (very few annotations given for a downstream), label shift (evaluation recordings come from different geographical regions), and covariate shift (difference in recording settings and environmental conditions of the same classes between pretraining and evaluation data).</p>\n<p><b>Bio</b>: Ilyass Moummad is a PhD student (December 2021 - November 2024) at IMT Atlantique, Brest, France. He works under the supervision of Nicolas Farrugia and is co-supervised by Romain Serizel. Currently, Ilyass is a Visiting Researcher at C4DM, QMUL, working under the supervision of Emmanouil Benetos (April - June 2024). Ilyass’s PhD topic is Deep Learning for Bioacoustics, with an interest in representation learning (both self-supervised and supervised) of animal sounds, as well as few-shot learning (species sound classification and detection from very few annotated examples).</p>","id":"eff5e87a-d691-5412-a7b5-cde72a465f19"},{"fields":{"slug":"/news/news.2024-06-07.C4DM-Eddie_Dobson"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar: Eddie Dobson","author":"Admin","date":"Tue 18 Jun 2024"},"html":"<h3>C4DM Seminar: Eddie Dobson: A social psychology approach to fostering inclusion in music technology education</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong><br>\nEddie Dobson</p>\n<p><strong>Date/time:  Friday, June 7, 2024, 11:00 AM - 12:00 PM</strong></p>\n<p>**Location: Graduate Centre, Room GC601, Mile End **</p>\n<p><b>Title</b>: A social psychology approach to fostering inclusion in music technology education</p>\n<hr>\n<p><b>Abstract</b>: Organised by C4DM/AIM/EECS/WHEN, in this talk Dr Eddie Dobson will address the considerable challenges facing Women and minority gender professionals (WMGP) in the audio industries, which are are serviced by a majority white, cis, able-bodied male workforce. The talk will touch on:</p>\n<ul>\n<li>the hidden curriculum,</li>\n<li>a gender-blind and technology-led understanding of engagement,</li>\n<li>the influence of early years access on engagement,</li>\n<li>a developmental perspective on identity and engagement in music technology,</li>\n<li>how learners are perceived by peers and educators 'confident boys and 'bolshie' girls,</li>\n<li>and the potential benefit of fun, friendship and collaborative play for supporting sustained engagement in MTE so that all learners might flourish.</li>\n</ul>\n<p><b>Bio</b>: Dr Eddie Dobson is an academic, composer and sound designer based at The University of Huddersfield. Eddie is a National Teaching Fellow and Principle Enterprise Fellow and has delivered many workshops for girls, women and minority gender learners at all levels of practice and is set to be a Key Tutor on the 2024 Sound and Music In the Making summer school. Developing a portfolio of audio work, Eddie has delivered audio-post production services and sound design for film and television since 2019, including work for Ch4 and Audible. Their latest acousmatic piece was premiered at a CeReNeM Loudspeaker Orchestra Concert in Greenwich in 2023, and performed at Electric Spring Festival in 2024.</p>\n<p>This event is part of the Pride Month 2024: <a href=\"https://www.qmul.ac.uk/pride-month/pride-month-2024\">https://www.qmul.ac.uk/pride-month/pride-month-2024</a></p>","id":"96f8ebab-99d1-520b-a159-ac3d564b26c3"},{"fields":{"slug":"/news/news.2024-06-18.C4DM-Mikolaj_Kegler"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png","srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/30cdc/placeholder.png 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/c7240/placeholder.png 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/e8b76/placeholder.png 1200w","sizes":"(min-width: 1200px) 1200px, 100vw"},"sources":[{"srcSet":"/static/6e45d55cd3da3f0a9c12568e44160cff/078c3/placeholder.webp 300w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/6d09e/placeholder.webp 600w,\n/static/6e45d55cd3da3f0a9c12568e44160cff/83805/placeholder.webp 1200w","type":"image/webp","sizes":"(min-width: 1200px) 1200px, 100vw"}]},"width":1200,"height":1200}}},"title":"C4DM Seminar: Mikolaj Kegler","author":"Admin","date":"Tue 18 Jun 2024"},"html":"<h3>C4DM Seminar: Mikolaj Kegler: Hear What You Want, towards seamless, immersive AI experiences for wearable audio devices</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong><br>\nMikolaj Kegler</p>\n<p><strong>Date/time:  Tuesday, 18th June 2024, 3pm</strong></p>\n<p>**Location: GC205, Greduate Centre, QMUL, E1 4NS **\nZoom: <a href=\"https://qmul-ac-uk.zoom.us/j/9798452959\">https://qmul-ac-uk.zoom.us/j/9798452959</a></p>\n<p><b>Title</b>: Hear What You Want, towards seamless, immersive AI experiences for wearable audio devices</p>\n<hr>\n<p><b>Abstract</b>: Hear What You Want is both the mission statement and the title of the Bose Research franchise established to build AI-powered experiences for next-generation wearable audio devices. This interdisciplinary franchise involves a wide range of research tracks, such as source separation and sound event detection, to name a few. All these efforts share one common denominator, the resulting solutions must be suitable for low-latency execution on the embedded target devices to enable seamless, immersive experiences. In this talk, I will start by outlining the fundamental problem of limited computational resources on embedded platforms and present our latest methods for building compact yet performant ML models. Subsequently, I will discuss the problem of low-latency causal target source extraction and highlight our recent developments. I will conclude by presenting our latest work on improving the efficiency and fidelity of text-toaudio generative models for Foley sound synthesis.</p>\n<p><b>Bio</b>: Mikolaj “Miko” Kegler is an Audio Machine Learning Scientist at Bose Corporation. His research at the interface of ML and DSP is focused on developing methods for lightweight, low-latency, on-device speech and audio signal processing. Before joining Bose, Miko was awarded a PhD from the Department of Bioengineering &#x26; Centre for Neurotechnology, Imperial College London, where he has been investigating neural mechanisms underlying perception and comprehension of speech, especially in challenging listening conditions. During his PhD, he completed multiple audio ML research placements, including at Amazon Lab126 and Logitech, where he also served as a long-term scientific consultant.</p>","id":"e286df2d-7792-5cdd-b6ac-2810abc3f1d0"},{"fields":{"slug":"/news/2024-06-03.C4DM-PhD_student_predicts_Eurovision_winner"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#b8a898","images":{"fallback":{"src":"/static/234763aa1e5f363371e4bfff769cc48d/70fef/Katarzyna-Adamska.jpg","srcSet":"/static/234763aa1e5f363371e4bfff769cc48d/463c6/Katarzyna-Adamska.jpg 379w,\n/static/234763aa1e5f363371e4bfff769cc48d/53da6/Katarzyna-Adamska.jpg 758w,\n/static/234763aa1e5f363371e4bfff769cc48d/70fef/Katarzyna-Adamska.jpg 1516w","sizes":"(min-width: 1516px) 1516px, 100vw"},"sources":[{"srcSet":"/static/234763aa1e5f363371e4bfff769cc48d/d4721/Katarzyna-Adamska.webp 379w,\n/static/234763aa1e5f363371e4bfff769cc48d/6420f/Katarzyna-Adamska.webp 758w,\n/static/234763aa1e5f363371e4bfff769cc48d/79063/Katarzyna-Adamska.webp 1516w","type":"image/webp","sizes":"(min-width: 1516px) 1516px, 100vw"}]},"width":1516,"height":1516}}},"title":"C4DM PhD student predicts Eurovision winner","author":"Emmanouil Benetos","date":"Mon 03 Jun 2024"},"html":"<p>Using a prediction tool, <a href=\"https://www.qmul.ac.uk/eecs/people/profiles/adamskakatarzynamaria.html\">Katarzyna Adamska</a>, a third year PhD student, correctly predicted that Switzerland’s song entry, ‘The Code’, would win this year’s competition.</p>\n<p>Read the full story in the EECS website: <a href=\"https://www.qmul.ac.uk/eecs/news-and-events/news/items/eecs-phd-researcher-predicts-eurovision-winner-.html\">https://www.qmul.ac.uk/eecs/news-and-events/news/items/eecs-phd-researcher-predicts-eurovision-winner-.html</a></p>","id":"5d4a7783-43de-5e4f-8185-8b86347998fd"},{"fields":{"slug":"/news/2024-06-02.C4DM-PhD_student_gives_talk_at_GDL_workshop"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/ca433a7002ce0a87e1a49ef7a6a0d116/b5658/m4dl.png","srcSet":"/static/ca433a7002ce0a87e1a49ef7a6a0d116/acb7c/m4dl.png 256w,\n/static/ca433a7002ce0a87e1a49ef7a6a0d116/ccc41/m4dl.png 512w,\n/static/ca433a7002ce0a87e1a49ef7a6a0d116/b5658/m4dl.png 1024w","sizes":"(min-width: 1024px) 1024px, 100vw"},"sources":[{"srcSet":"/static/ca433a7002ce0a87e1a49ef7a6a0d116/22bfc/m4dl.webp 256w,\n/static/ca433a7002ce0a87e1a49ef7a6a0d116/d689f/m4dl.webp 512w,\n/static/ca433a7002ce0a87e1a49ef7a6a0d116/67ded/m4dl.webp 1024w","type":"image/webp","sizes":"(min-width: 1024px) 1024px, 100vw"}]},"width":1024,"height":1024}}},"title":"C4DM PhD student to present at the Geometric Deep Learning workshop","author":"Emmanouil Benetos","date":"Sun 02 Jun 2024"},"html":"<p>Centre for Digital Music PhD student <a href=\"https://shubhrsingh22.github.io/\">Shubhr Singh</a> will be presenting his recent work on \"Local-Higher Order GNNs For Audio Classification And Tagging\" as part of the <a href=\"https://maths4dl.ac.uk/newsevents/geometric-deep-learning-workshop-university-of-cambridge-10-12-june-2024\">Geometric Deep Learning workshop</a>, taking place on 10-12 June 2024 at the University of Cambridge. The workshop is organised by the EPSRC's <a href=\"https://maths4dl.ac.uk/\">Mathematics for Deep Learning programme grant (m4DL)</a>.</p>\n<p>Shubhr's work introduces the Local-Higher Order Graph Neural Network (LHGNN), which stands as a significant step forward in the field of audio classification and tagging, providing a robust framework that leverages graph-based and clustering methodologies to achieve high performance.</p>","id":"9df4d5da-9aa9-51b1-9cc8-2e7941da10ad"},{"fields":{"slug":"/news/2024-05-29.C4DM-PhD_students_organise_DCASE_task"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#282828","images":{"fallback":{"src":"/static/1778669a3aa300652155162f4abc6354/0a9b5/DCASE.jpg","srcSet":"/static/1778669a3aa300652155162f4abc6354/384e6/DCASE.jpg 292w,\n/static/1778669a3aa300652155162f4abc6354/75c79/DCASE.jpg 584w,\n/static/1778669a3aa300652155162f4abc6354/0a9b5/DCASE.jpg 1167w","sizes":"(min-width: 1167px) 1167px, 100vw"},"sources":[{"srcSet":"/static/1778669a3aa300652155162f4abc6354/ad81e/DCASE.webp 292w,\n/static/1778669a3aa300652155162f4abc6354/4e6da/DCASE.webp 584w,\n/static/1778669a3aa300652155162f4abc6354/c3035/DCASE.webp 1167w","type":"image/webp","sizes":"(min-width: 1167px) 1167px, 100vw"}]},"width":1167,"height":1167}}},"title":"C4DM PhD students organise DCASE challenge task on computational bioacoustics","author":"Emmanouil Benetos","date":"Wed 29 May 2024"},"html":"<p>Centre for Digital Music PhD students <a href=\"https://scholar.google.com/citations?user=C1jftogAAAAJ&#x26;hl=en\">Ines Nolasco</a>, <a href=\"https://shubhrsingh22.github.io/\">Shubhr Singh</a>, and <a href=\"https://jinhualiang.github.io/\">Jinhua Liang</a> are organising the task on <a href=\"https://dcase.community/challenge2024/task-few-shot-bioacoustic-event-detection\">Few-shot Bioacoustic Event Detection</a> as part of the IEEE AASP Challenge on Detection and Classification of Acoustic Scenes and Events (DCASE 2024).</p>\n<p>This task addresses a real need from animal researchers by providing a well-defined, constrained yet highly variable domain to evaluate machine learning methodology. It aims to advance the study of audio signal processing and deep learning in the low-resource scenario, particularly in domain adaptation and few-shot learning. Datasets will be released on 1st June 2024, with the challenge deadline being on 15 June 2024.</p>\n<p>Can you build a system that detects an animal sound with only 5 examples?  Let's liaise to push the boundary of computational bioacoustics and machine listening!</p>","id":"cfaa6a83-ed54-520e-8c11-4732a55fab01"}]}}}