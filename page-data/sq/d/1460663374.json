{"data":{"about":{"html":"<p>The <em><strong>Centre for Digital Music</strong></em> is a world-leading, multidisciplinary research group in the field of music &#x26; audio technology.</p>\n<!-- <p style=\"color: red\">**THIS SEEMS TO BE THE ONLY SUPPORTED WAY OF IMPORTING TWITTER TWEETS IN GATSBY 5 AT THE MOMENT, AFAIK IT MEANS THAT EACH \nTWEET HAS TO BE MANUALLY IMPORTED INTO AN MD FILE...**</p>\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">This is what we do: <a href=\"https://t.co/pkyS6IcIUy\">https://t.co/pkyS6IcIUy</a> - an excellent video intro to the wonderful researchers of <a href=\"https://twitter.com/c4dm?ref_src=twsrc%5Etfw\">@c4dm</a>. <a href=\"https://twitter.com/QMEECS?ref_src=twsrc%5Etfw\">@QMEECS</a> <a href=\"https://twitter.com/QMUL?ref_src=twsrc%5Etfw\">@QMUL</a> <a href=\"https://twitter.com/hashtag/research?src=hash&amp;ref_src=twsrc%5Etfw\">#research</a></p>&mdash; C4DM at QMUL (@c4dm) <a href=\"https://twitter.com/c4dm/status/857989625922695173?ref_src=twsrc%5Etfw\">April 28, 2017</a></blockquote> -->","frontmatter":{"title":"About the centre","video":"https://www.youtube.com/embed/Rcbs4NvMFHM","image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png","srcSet":"/static/c138fbce66e709a3f503405435de2f2c/0fc07/c4dm.png 100w,\n/static/c138fbce66e709a3f503405435de2f2c/bc685/c4dm.png 200w,\n/static/c138fbce66e709a3f503405435de2f2c/354fd/c4dm.png 400w,\n/static/c138fbce66e709a3f503405435de2f2c/7ef25/c4dm.png 800w","sizes":"(min-width: 400px) 400px, 100vw"},"sources":[{"srcSet":"/static/c138fbce66e709a3f503405435de2f2c/29e9d/c4dm.webp 100w,\n/static/c138fbce66e709a3f503405435de2f2c/2f9ed/c4dm.webp 200w,\n/static/c138fbce66e709a3f503405435de2f2c/ece87/c4dm.webp 400w,\n/static/c138fbce66e709a3f503405435de2f2c/4bc26/c4dm.webp 800w","type":"image/webp","sizes":"(min-width: 400px) 400px, 100vw"}]},"width":400,"height":60}}}}},"projects":{"nodes":[{"fields":{"slug":"/projects/cdtdata"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080838","images":{"fallback":{"src":"/static/bd2eecf83bf5392a4ac7061919a52914/cd63e/cdtdata.png","srcSet":"/static/bd2eecf83bf5392a4ac7061919a52914/08744/cdtdata.png 145w,\n/static/bd2eecf83bf5392a4ac7061919a52914/3c29b/cdtdata.png 290w,\n/static/bd2eecf83bf5392a4ac7061919a52914/cd63e/cdtdata.png 579w","sizes":"(min-width: 579px) 579px, 100vw"},"sources":[{"srcSet":"/static/bd2eecf83bf5392a4ac7061919a52914/72079/cdtdata.webp 145w,\n/static/bd2eecf83bf5392a4ac7061919a52914/a7f7a/cdtdata.webp 290w,\n/static/bd2eecf83bf5392a4ac7061919a52914/ba6fb/cdtdata.webp 579w","type":"image/webp","sizes":"(min-width: 579px) 579px, 100vw"}]},"width":579,"height":579}}},"title":"Centre for Doctoral Training (CDT) in Data Centric Engineering","author":"Prof Eram Rizvi (PI), Prof Mark Sandler (CI), Prof Nick Bryan-Kinns (CI)","date":null},"html":"","id":"e68ade9f-702f-5695-9a7d-e2ab94af8c72"},{"fields":{"slug":"/projects/Fazekas-Sony"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"New Methodologies for Efficient and Controllable Music Generation","author":"Dr George Fazekas (PI)","date":null},"html":"","id":"a3e9f3a1-9edd-50fe-b941-283fe6f7de85"},{"fields":{"slug":"/projects/Fazekas-UMG2"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Beyond Supervised Learning for Musical Audio","author":"Dr George Fazekas (PI)","date":null},"html":"","id":"433744e4-ecdf-538d-903d-85748cab2ea6"},{"fields":{"slug":"/projects/aimcdt"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/98704461ac8eeac7d8b8a64dc1520bf4/7a590/aimcdt.png","srcSet":"/static/98704461ac8eeac7d8b8a64dc1520bf4/4b686/aimcdt.png 154w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/d2213/aimcdt.png 308w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/7a590/aimcdt.png 615w","sizes":"(min-width: 615px) 615px, 100vw"},"sources":[{"srcSet":"/static/98704461ac8eeac7d8b8a64dc1520bf4/b2942/aimcdt.webp 154w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/46581/aimcdt.webp 308w,\n/static/98704461ac8eeac7d8b8a64dc1520bf4/fa942/aimcdt.webp 615w","type":"image/webp","sizes":"(min-width: 615px) 615px, 100vw"}]},"width":615,"height":615}}},"title":"UKRI Centre for Doctoral Training in Artificial Intelligence and Music (AIM)","author":"Prof Simon Dixon (PI), Dr Mathieu Barthet (CI), Dr Nick Bryan-Kinns (CI), Dr Gyorgy Fazekas (CI), Prof Mark Sandler (CI), Dr Andrew McPherson (CI), Dr Emmanouil Benetos (CI)","date":null},"html":"","id":"237af1c8-14dc-5167-a5e1-fa55e6860dea"},{"fields":{"slug":"/projects/Dixon-DAACI"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Performance Rendering for Music Generation Systems","author":"Prof Simon Dixon (PI)","date":null},"html":"","id":"780d9bfc-fda8-5320-aa0c-0a93788ae549"},{"fields":{"slug":"/projects/Benetos-Spotify"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#4848a8","images":{"fallback":{"src":"/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png","srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/de3a1/rudiments.png 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/30cdc/rudiments.png 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/6c0ff/rudiments.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/667dfeac59460a16f71a264dfc9745d7/c65bc/rudiments.webp 150w,\n/static/667dfeac59460a16f71a264dfc9745d7/078c3/rudiments.webp 300w,\n/static/667dfeac59460a16f71a264dfc9745d7/3b6e5/rudiments.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"Style classification of podcasts using audio","author":"Dr Emmanouil Benetos (PI)","date":null},"html":"","id":"6838b490-b59d-5848-808c-d2f8f734d0c6"}]},"news":{"nodes":[{"fields":{"slug":"/news/2024-04-06.C4DM-at_ICASSP_2024"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/8407702d136537b5f2f75c5a80fedd21/47930/ICASSP2024.jpg","srcSet":"/static/8407702d136537b5f2f75c5a80fedd21/e07e1/ICASSP2024.jpg 100w,\n/static/8407702d136537b5f2f75c5a80fedd21/dd515/ICASSP2024.jpg 200w,\n/static/8407702d136537b5f2f75c5a80fedd21/47930/ICASSP2024.jpg 400w","sizes":"(min-width: 400px) 400px, 100vw"},"sources":[{"srcSet":"/static/8407702d136537b5f2f75c5a80fedd21/d8057/ICASSP2024.webp 100w,\n/static/8407702d136537b5f2f75c5a80fedd21/2e34e/ICASSP2024.webp 200w,\n/static/8407702d136537b5f2f75c5a80fedd21/416c3/ICASSP2024.webp 400w","type":"image/webp","sizes":"(min-width: 400px) 400px, 100vw"}]},"width":400,"height":400}}},"title":"C4DM at ICASSP 2024","author":"Emmanouil Benetos","date":"Sat 06 Apr 2024"},"html":"<p>On 14-19 April, several C4DM researchers will participate at the <b><a href=\"https://2024.ieeeicassp.org/\">2024 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP 2024)</a></b>. ICASSP is the leading conference in the field of signal processing and the flagship event of the <a href=\"https://signalprocessingsociety.org/\">IEEE Signal Processing Society</a>.</p>\n<p>As in previous years, the Centre for Digital Music will have a strong presence at the conference, both in terms of numbers and overall impact. The below papers presented at ICASSP 2024 are authored or co-authored by C4DM members:</p>\n<ul>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/10446182\">High resolution guitar transcription via domain adaptation</a>, by Xavier Riley, Drew Edwards, and Simon Dixon</p>\n</li>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/10446908\">Learning from taxonomy: multi-label few-shot classification for everyday sound recognition</a>, by Jinhua Liang, Huy Phan, and Emmanouil Benetos</p>\n</li>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/10446400\">Bass accompaniment generation via latent diffusion</a>, by Marco Pasini, Maarten Grachten, and Stefan Lattner</p>\n</li>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/10447564\">Unsupervised pitch-timbre disentanglement of musical instruments using a Jacobian disentangled sequential autoencoder</a>, by Yin-Jyun Luo, Sebastian Ewert, and Simon Dixon</p>\n</li>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/10447835\">Posterior variance-parameterised Gaussian dropout: improving disentangled sequential autoencoders for zero-shot voice conversion</a>, by Yin-Jyun Luo and Simon Dixon</p>\n</li>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/10447445\">Mertech: instrument playing technique detection using self-supervised pretrained model with multi-task finetuning</a>, by Dichucheng Li, Yinghao Ma, Weixing Wei, Qiuqiang Kong, Yulun Wu, Mingjin Che, Fan Xia, Emmanouil Benetos, and Wei Li</p>\n</li>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/10447122\">Generalized multi-source inference for text conditioned music diffusion models</a>, by Emilian Postolache, Giorgio Mariani, Luca Cosmo, Emmanouil Benetos, and Emanuele Rodolà</p>\n</li>\n<li>\n<p><a href=\"https://ieeexplore.ieee.org/document/10447063\">Syncfusion: Multimodal Onset-Synchronized Video-to-Audio Foley Synthesis</a>, by Marco Comunità, Riccardo F. Gramaccioni, Emilian Postolache, Emanuele Rodolà, Danilo Comminiello, and Joshua D. Reiss</p>\n</li>\n</ul>\n<p>The following paper will be presented at the <a href=\"https://xai-sa-workshop.github.io/web/Home.html\">ICASSP Workshop on Explainable Machine Learning for Speech and Audio</a>:</p>\n<ul>\n<li><a href=\"https://www.researchgate.net/publication/379085262_Explainable_Modeling_of_Gender-Targeting_Practices_in_Toy_Advertising_Sound_and_Music\">Explainable modeling of gender-targeting practices in toy advertising sound and music</a>, by Luca Marinelli and Charalampos Saitis</li>\n</ul>\n<p>See you all at ICASSP!</p>","id":"b75a7542-b72f-56b1-9e20-2bcf429b1e64"},{"fields":{"slug":"/news/2023-05-26.C4DM-at_ICMPC_2023"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/2b6f0fee66cf2e8e3786de08167a40f2/ca426/ICMPC2023.png","srcSet":"/static/2b6f0fee66cf2e8e3786de08167a40f2/bb5d0/ICMPC2023.png 126w,\n/static/2b6f0fee66cf2e8e3786de08167a40f2/2cc56/ICMPC2023.png 253w,\n/static/2b6f0fee66cf2e8e3786de08167a40f2/ca426/ICMPC2023.png 505w","sizes":"(min-width: 505px) 505px, 100vw"},"sources":[{"srcSet":"/static/2b6f0fee66cf2e8e3786de08167a40f2/fabbe/ICMPC2023.webp 126w,\n/static/2b6f0fee66cf2e8e3786de08167a40f2/2d900/ICMPC2023.webp 253w,\n/static/2b6f0fee66cf2e8e3786de08167a40f2/0b3af/ICMPC2023.webp 505w","type":"image/webp","sizes":"(min-width: 505px) 505px, 100vw"}]},"width":505,"height":505}}},"title":"C4DM at ICMPC 2023","author":"Admin","date":"Fri 26 May 2023"},"html":"<p>From 24 to 28 August, several C4DM researchers will participate in the <b><a href=\"https://jsmpc.org/ICMPC17/\">17th International Conference on Music Perception and Cognition (ICMPC 2023)</a></b>.</p>\n<p>Below is a list of all the contributions authored or co-authored by C4DM members:</p>\n<ul>\n<li>\n<p><strong>Analysing the Gendering of Music in Toy Commercials via Mid-level Perceptual Features</strong> by Luca Marinelli, and Charalampos Saitis</p>\n</li>\n<li>\n<p><strong>Exploring the Role of Audio and Lyrics in Explaining Moral Worldviews</strong> by Vjosa Preniqi, Kyriaki Kalimeri, and Charalampos Saitis</p>\n</li>\n<li>\n<p><strong>Evolution of Moral Valence in Lyrics Over Time</strong> by Vjosa Preniqi, Kyriaki Kalimeri, Andreas Kaltenbrunner, and Charalampos Saitis</p>\n</li>\n<li>\n<p><strong>Modelling the perception of large-scale order in music</strong> by Edward Hall and Marcus Pearce</p>\n</li>\n<li>\n<p><strong>The Billboard Melodic Music Dataset and Trajectories in Western Pop Music</strong> by Madeline Hamilton and Marcus Pearce</p>\n</li>\n<li>\n<p><strong>Informational constraints and trade-offs in melodies across cultures</strong> by John McBride, Marcus Pearce and Tsvi Tlusty</p>\n</li>\n<li>\n<p><strong>The Cultural Distance Hypothesis and its Impact on Melodic Perception</strong> by Mathias Klarlund, Elvira Brattico, Yi Du, Marcus Pearce, Peter Vuust, Morten Overgaard and Yiyang Wu</p>\n</li>\n<li>\n<p><strong>Predictive processes shape the relation between groove and syncopation</strong> by Tomas Matthews, Jonathan Cannon, Victor Pando-Naude, Jan Stupacher, Isaac Romkey, Thomas Kaplan, Gunvor Bertelsen, Alexandre Celma Miralles, Virginia Penhune and Peter Vuust</p>\n</li>\n</ul>\n<p>The following symposia are co-organised by C4DM members:</p>\n<ul>\n<li>\n<p><strong>Symposium: Predictions in Music: Neurocognitive Insights and Computational Approaches</strong> by Vincent Cheung, Peter Harrison, Psyche Loui, Marcus Pearce and Daniela Sammler</p>\n</li>\n<li>\n<p><strong>Symposium: Modelling rhythm perception beyond the beat</strong> by Atser Damsma, Fleur Bouwer, Lauren Fink, Jonathan Cannon, Keith Doelling, Jessica Grahn, Henkjan Honing and Thomas Kaplan</p>\n</li>\n</ul>\n<p>Happy ICMPCing!</p>","id":"a0eca885-ec21-52d4-bb86-4ee40d56ae4f"},{"fields":{"slug":"/news/2023-05-26.C4DM-at_NIME_2023"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#080808","images":{"fallback":{"src":"/static/4eb8fc854adc4eb2ac813efcfae5b7ef/6c0ff/nime2023.png","srcSet":"/static/4eb8fc854adc4eb2ac813efcfae5b7ef/de3a1/nime2023.png 150w,\n/static/4eb8fc854adc4eb2ac813efcfae5b7ef/30cdc/nime2023.png 300w,\n/static/4eb8fc854adc4eb2ac813efcfae5b7ef/6c0ff/nime2023.png 599w","sizes":"(min-width: 599px) 599px, 100vw"},"sources":[{"srcSet":"/static/4eb8fc854adc4eb2ac813efcfae5b7ef/c65bc/nime2023.webp 150w,\n/static/4eb8fc854adc4eb2ac813efcfae5b7ef/078c3/nime2023.webp 300w,\n/static/4eb8fc854adc4eb2ac813efcfae5b7ef/3b6e5/nime2023.webp 599w","type":"image/webp","sizes":"(min-width: 599px) 599px, 100vw"}]},"width":599,"height":599}}},"title":"C4DM at NIME 2023","author":"Admin","date":"Fri 26 May 2023"},"html":"<p>From 31 May to 3 June, several C4DM researchers will participate in the <b><a href=\"http://nime2023.org/\">2023 International Conference on New Interfaces for Musical Expression (NIME 2023)</a></b>. Once again, this year's edition will have <b><a href=\"https://bela.io/\">Bela</a></b> as an official sponsor.</p>\n<p>Below is a list of all the publications authored or co-authored by C4DM members:</p>\n<ul>\n<li>\n<p><a href=\"\">Instructions not included: Dementia Friendly approaches to DMI Design</a> by Jon Pigrem, Jennifer Macritchie, and Andrew McPherson</p>\n</li>\n<li>\n<p><a href=\"https://sebastianlobbers.com/static/9324af8a968715544e88586037601b98/SketchSynth_Lobbers_NIME.pdf\">SketchSynth: a browser-based sketching interface for sound control</a> by Sebastian Löbbers and György Fazekas</p>\n</li>\n<li>\n<p><a href=\"https://arxiv.org/abs/2305.14867\">Interactive Neural Resonators</a> by Rodrigo Diaz, Charalampos Saitis, and Mark Sandler</p>\n</li>\n<li>\n<p><a href=\"https://teodannemann.files.wordpress.com/2023/05/nime2023_disability-2.pdf\">Music jamming as a participatory design method. A case study with disabled musicians</a> by Teodoro Dannemann</p>\n</li>\n<li>\n<p><a href=\"https://teodannemann.files.wordpress.com/2023/05/nime2023_wip-5.pdf\">The Sabotaging Piano: key-to-pitch remapping as a source of new techniques in piano improvisation</a> by Teodoro Dannemann and Nick Bryan-Kinns</p>\n</li>\n<li>\n<p><a href=\"https://teodannemann.files.wordpress.com/2023/05/nime2023_final-4.pdf\">Self-Sabotage Workshop: a starting point to unravel sabotaging of instruments as a design practice</a> by Teodoro Dannemann, Nick Bryan-Kinns, Andrew McPherson</p>\n</li>\n<li>\n<p><a href=\"https://teodannemann.files.wordpress.com/2023/05/nime2023_music-1.pdf\">Sabotaging Piano Concert (Music Submission)</a> by Teodoro Dannemann</p>\n</li>\n<li>\n<p><a href=\"\">Exploring the (un)ambiguous guitar: A Qualitative Study on the use of Gesture Disambiguation in Augmented Instrument Design</a> by Adan L. Benito Temprano, Teodoro Dannemann, and Andrew P. McPherson</p>\n</li>\n<li>\n<p><a href=\"https://www.teresapelinski.com/documents/2023-nime-cr-pipeline-nn-bela-v3.pdf\">Pipeline for recording datasets and running neural networks on the Bela embedded hardware platform</a> by Teresa Pelinski, Rodrigo Diaz,  Adan L. Benito Temprano, Andrew McPherson</p>\n</li>\n</ul>\n<p>The following workshop is co-organised by C4DM members:</p>\n<ul>\n<li><a href=\"https://qe4nime.github.io/\">Querying Experience with Musical Interaction Workshop</a> by Courtney N. Reed, Eevee Zayas-Garin, and Andrew McPherson</li>\n</ul>\n<p>Happy NIMEing!</p>","id":"458b680f-64c2-5d74-8883-41b17772c24b"},{"fields":{"slug":"/news/2022-09-22.C4DM-students_join_the_Turing"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#f8f8f8","images":{"fallback":{"src":"/static/b14595a1cf9cbbf96f00ff2d37df1499/621f0/turing.png","srcSet":"/static/b14595a1cf9cbbf96f00ff2d37df1499/37ed9/turing.png 121w,\n/static/b14595a1cf9cbbf96f00ff2d37df1499/a3c30/turing.png 242w,\n/static/b14595a1cf9cbbf96f00ff2d37df1499/621f0/turing.png 484w","sizes":"(min-width: 484px) 484px, 100vw"},"sources":[{"srcSet":"/static/b14595a1cf9cbbf96f00ff2d37df1499/368a5/turing.webp 121w,\n/static/b14595a1cf9cbbf96f00ff2d37df1499/ed101/turing.webp 242w,\n/static/b14595a1cf9cbbf96f00ff2d37df1499/6e7b2/turing.webp 484w","type":"image/webp","sizes":"(min-width: 484px) 484px, 100vw"}]},"width":484,"height":484}}},"title":"C4DM students to join the Alan Turing Institute in 2022/23","author":"Admin","date":"Wed 15 Feb 2023"},"html":"<p>Two C4DM PhD students have been given <a href=\"https://www.turing.ac.uk/work-turing/studentships/enrichment\">enrichment awards</a> by the <a href=\"https://www.turing.ac.uk/\">Alan Turing Institute</a>, the UK’s national institute in artificial intelligence and data science, enabling them to join and interact with institute researchers and its community in the 2022/23 academic year.</p>\n<p>Specifically, C4DM PhD student <a href=\"http://eecs.qmul.ac.uk/profiles/banarberker.html\">Berker Banar</a> has been offered a Turing Enrichment Community Award for the project “Towards Composing Contemporary Classical Music using Generative Deep Learning” and C4DM PhD student <a href=\"http://eecs.qmul.ac.uk/profiles/huangjiawen-1.html\">Jiawen Huang</a> has been offered an Enrichment Placement Award for the project “Real-Time Audio-to-Lyrics Alignment for Polyphonic Music”.</p>\n<p>Congratulations to both! For the full story on enrichment awards for Queen Mary doctoral students please read the <a href=\"https://www.qmul.ac.uk/media/news/2022/pr/queen-mary-students-to-gain-valuable-research-experience-through-placements-at-the-alan-turing-institute-.html\">QMUL newsitem</a>.</p>\n<p><i>(22 September 2022)</i></p>","id":"f8575e94-ef6e-5f1a-84cf-51750ffe256a"},{"fields":{"slug":"/news/2022-10-11.C4DM-Seminar_-_Vipul_Arora_-_Model_Adaptation_for_Learning_from_Small_Data"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#e8e8e8","images":{"fallback":{"src":"/static/6028f1c51e904116cff0165dba062e8d/b988d/vipulaurora.jpg","srcSet":"/static/6028f1c51e904116cff0165dba062e8d/f18e1/vipulaurora.jpg 96w,\n/static/6028f1c51e904116cff0165dba062e8d/0caa4/vipulaurora.jpg 193w,\n/static/6028f1c51e904116cff0165dba062e8d/b988d/vipulaurora.jpg 385w","sizes":"(min-width: 385px) 385px, 100vw"},"sources":[{"srcSet":"/static/6028f1c51e904116cff0165dba062e8d/8ef24/vipulaurora.webp 96w,\n/static/6028f1c51e904116cff0165dba062e8d/6a483/vipulaurora.webp 193w,\n/static/6028f1c51e904116cff0165dba062e8d/daff6/vipulaurora.webp 385w","type":"image/webp","sizes":"(min-width: 385px) 385px, 100vw"}]},"width":385,"height":385}}},"title":"C4DM Seminar: Dr. Vipul Arora","author":"Jan Novak","date":"Wed 15 Feb 2023"},"html":"<h3>C4DM Seminar: Dr. Vipul Arora</h3>\n<hr>\n<h4>QMUL, School of Electronic Engineering and Computer Science</h4>\n<h4>Centre for Digital Music Seminar Series</h4>\n<p><strong>Seminar by:</strong><br>\nVipul Arora</p>\n<p><strong>Date/time: Wednesday 11th of October, 14:00-15:00</strong></p>\n<p><strong>Location: Hybrid</strong></p>\n<p>Bancroft Road Teaching Room 4.01 in the Mile End campus, or join <a href=\"https://qmul-ac-uk.zoom.us/j/84254407411?pwd=azU2WU1ZWTIvN0pEb3lQZVg2SVNNQT09\">zoom meeting</a>.</p>\n<p>Open to students, staff, alumni, public; all welcome.\nAdmission is FREE, no pre-booking required.</p>\n<hr>\n<p><b>Title</b>: Model Adaptation for Learning from Small Data</p>\n<p><b>Bio</b>:\nVipul Arora is an Associate Professor at the Department of Electrical Engineering, IIT Kanpur, India. He received his B.Tech. and PhD degrees in Electrical Engineering from IIT Kanpur. He did a postdoc at Oxford University (UK), where he developed speech recognition systems using linguistic principles. Then he worked at Amazon in Boston (USA), where he worked on audio classification for developing the Alexa home security system. His current research interest is in developing machine learning algorithms for audio and signal processing. He works on model adaptation, uncertainty modelling and generative machine learning. <a href=\"http://home.iitk.ac.in/~vipular/\">http://home.iitk.ac.in/~vipular/</a></p>\n<p><b>Abstract</b>:\nDeep-learning-based models achieve remarkable performances with big labelled data. However, many practical scenarios face a scarcity of labelled data, while there may still be an abundance of unlabelled data. This talk will discuss several methods to learn effectively from small data. These methods mostly fall under the paradigm of model adaptation and include fine-tuning-based transfer learning, meta-learning, and semi-supervised domain adaptation. These methods’ application to music melody estimation and sensor calibration (regression) problems will be demonstrated. Another way to learn from limited data is by using conditional models. This method will be illustrated for generative machine learning applied to XY models in statistical Physics and field theories in Particle physics. Time permitting, there will be a brief presentation on audio retrieval.</p>\n<p><b>Video recording</b>: <a href=\"https://www.youtube.com/watch?v=-_AS8_NNtWw\">https://www.youtube.com/watch?v=-_AS8_NNtWw</a></p>","id":"3014d431-51d3-5057-8d64-bd1756c327c4"},{"fields":{"slug":"/news/2022-10-26.C4DM-at_DCASE_2022"},"frontmatter":{"image":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","backgroundColor":"#584828","images":{"fallback":{"src":"/static/0578136d23b2bff7943757288c183371/af44b/dcase2022.png","srcSet":"/static/0578136d23b2bff7943757288c183371/56197/dcase2022.png 108w,\n/static/0578136d23b2bff7943757288c183371/9b7db/dcase2022.png 217w,\n/static/0578136d23b2bff7943757288c183371/af44b/dcase2022.png 433w","sizes":"(min-width: 433px) 433px, 100vw"},"sources":[{"srcSet":"/static/0578136d23b2bff7943757288c183371/e26a9/dcase2022.webp 108w,\n/static/0578136d23b2bff7943757288c183371/87dc3/dcase2022.webp 217w,\n/static/0578136d23b2bff7943757288c183371/198e2/dcase2022.webp 433w","type":"image/webp","sizes":"(min-width: 433px) 433px, 100vw"}]},"width":433,"height":433}}},"title":"C4DM at DCASE 2022","author":"Jan Novak","date":"Wed 15 Feb 2023"},"html":"<p>On 3-4 November, several C4DM researchers will participate at the <b><a href=\"https://dcase.community/workshop2022/\">7th Workshop on Detection and Classification of Acoustic Scenes and Events (DCASE 2022)</a></b>.  The workshop aims to provide a venue for researchers working on computational analysis of sound events and scene analysis to present and discuss their results, and is organised in conjunction with the <a href=\"https://dcase.community/challenge2022/\">DCASE 2022 Challenge</a>.</p>\n<p>As in previous years, the Centre for Digital Music will have a strong presence at the workshop, both in terms of numbers and overall impact. The below papers presented at DCASE 2022 are authored or co-authored by C4DM members:</p>\n<ul>\n<li>\n<p><a href=\"https://qmro.qmul.ac.uk/xmlui/handle/123456789/82125\">Few-shot bioacoustic event detection: enhanced classifiers for prototypical networks</a>, by Ren Li, Jinhua Liang, Huy Phan</p>\n</li>\n<li>\n<p><a href=\"https://qmro.qmul.ac.uk/xmlui/handle/123456789/82109\">Leveraging label hierarchies for few-shot everyday sound recognition</a>, by Jinhua Liang, Huy Phan, Emmanouil Benetos</p>\n</li>\n<li>\n<p><a href=\"https://arxiv.org/abs/2207.07911\">Few-shot bioacoustic event detection at the DCASE 2022 challenge</a>, by Ines Nolasco, Dan Stowell, Vincent Lostanlen, Shubhr Singh, Veronica Morfi, Ari Strandburg-Peshkin, Lisa Gill, Emily Grout, Ester Vidana-Villa, Joe Morford, Michael Emmerson, Frants Jensen, Helen Whitehead, Hanna Pamula, Ivan Kiskin</p>\n</li>\n<li>\n<p><a href=\"https://www.turing.ac.uk/research/publications/explaining-decisions-anomalous-sound-detectors\">Explaining the decisions of anomalous sound detectors</a>, by Kimberly T. Mai, Toby Davies, Lewis D. Griffin, Emmanouil Benetos</p>\n</li>\n</ul>\n<p>On challenge organisation, C4DM PhD students <a href=\"https://scholar.google.com/citations?user=C1jftogAAAAJ\">Inês Nolasco</a> and <a href=\"http://eecs.qmul.ac.uk/profiles/singhshubhr.html\">Shubhr Singh</a>, QMUL research assistant <a href=\"https://www.qmul.ac.uk/sbbs/staff/michael-emmerson.html\">Michael Emmerson</a>, C4DM alumna and research visitor <a href=\"https://scholar.google.com/citations?user=8izRvu4AAAAJ\">Veronica Morfi</a>, and C4DM alumnus <a href=\"http://www.mcld.co.uk/research/\">Dan Stowell</a> are all involved in the organisation of the DCASE 2022 Challenge task on <a href=\"https://dcase.community/challenge2022/task-few-shot-bioacoustic-event-detection\">Few-shot Bioacoustic Event Detection</a>, focusing on sound event detection in a few-shot learning setting for animal (mammal and bird) vocalisations.</p>\n<p>See you all at DCASE!</p>","id":"058db84f-cf26-5e15-9378-807cb6f0e300"}]}}}